---
title: "Informe"
author:
- "Azahara Martínez, María de los Ángeles Díaz,Álvaro Nieva, Iyán Álvarez,"
- "Florencia Pellegrini, Óscar Camacho"
date: "2026-01-14"
output:
  pdf_document:
    citation_package: natbib
  html_document: default
bibliography: referencias.bib
link-citations: true
biblio-style: IEEEtran
header-includes:
  - \citestyle{numbers}
  - \setcitestyle{numbers,square}
  - \usepackage{float}
reference-section-title: "REFERENCIAS"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  fig.width = 7,   # Ancho global
  fig.height = 5,  # Alto global
  fig.align = "center",
  out.width = "80%" # Esto asegura que en el documento final no ocupen todo el ancho
)
```


```{r, include=FALSE}
# library(seewave)
# library(tuneR)
# library(caret)
# library(pROC)
# library(dplyr)
# library(randomForest)

if (!requireNamespace("pacman", quietly = TRUE)) install.packages("pacman")

pacman::p_load(
  dplyr, seewave, tuneR, caret, pROC, randomForest,ggplot2,nnet,av
)
```

## INTRODUCCIÓN Y OBJETIVOS DEL PROYECTO

La voz es una señal acústica compleja que contiene información tanto del contenido lingüístico como de características propias del hablante. Mediante técnicas de procesado digital de señales es posible transformar una grabación en un conjunto de descriptores numéricos que resumen propiedades relevantes del audio, y utilizar estos descriptores para resolver tareas de clasificación.

En este trabajo se plantea el análisis de señales de voz humanas y generadas por inteligencia artificial, con el fin de estudiar qué características acústicas permiten distinguir entre distintos tipos de hablantes y orígenes de la señal. 

Así pues, se centra en tres objetivos principales: la clasificación del sexo del hablante, la detección de voces generadas por inteligencia artificial frente a voces humanas, y la identificación del acento (español neutro, andaluz y argentino). Para abordar estos problemas, las señales se transformarán en conjuntos de datos estructurados que permiten el entrenamiento y evaluación de distintos modelos de clasificación.

## PREPROCESADO

### Conversión y lectura de audios

Como primer paso del preprocesado, se desarrolló una función para la conversión de archivos de audio desde el formato `.m4a` al formato `.wav`, con el objetivo de unificar el tipo de señal y facilitar su posterior análisis. Para ello, se emplearon las librerías `av` y `tuneR` de R, que permiten la conversión y lectura de señales de audio de forma eficiente.

La función implementada convierte cada archivo a una señal monofónica con una frecuencia de muestreo de 16 kHz y guarda los archivos resultantes en un directorio específico. Además, se automatizó el proceso para convertir de manera recursiva todos los archivos `.m4a` contenidos en una carpeta y sus subdirectorios. Los audios generados mediante voz sintética (Voz IA) no se incluyen en este proceso, ya que se encuentran originalmente en formato `.wav` y no requieren conversión adicional.

```{r,include=FALSE,eval=FALSE}
convertir_m4a_a_wave <- function(input_m4a, sample_rate = 16000) {
 #Creamos la carpeta donde guardaremos los audios.
  dir.create("Audios_finales_conv", showWarnings = FALSE) 
  #Le damos nombre al archivo
  base_name <- tools::file_path_sans_ext(basename(input_m4a)) 
  output_wav <- file.path("Audios_finales_conv", paste0(base_name, ".wav"))
  
  av_audio_convert(
    input_m4a,        #archivo de entrada
    output_wav,       #archivo de salida
    channels    = 1,
    sample_rate = sample_rate
  )
  
  return(output_wav)
}

#Carpeta donde están los audios originalmente
input_dir <- "Audios_finales_orig"

m4a_files <- list.files(
  path       = input_dir,
  pattern    = "\\.m4a$",
  recursive  = TRUE,     
  full.names = TRUE
)

rutas_wav <- sapply(m4a_files, convertir_m4a_a_wave)
```

### Detección y eliminación de ruido y silencios

Con el objetivo de mejorar la calidad de la base de datos y eliminar segmentos de audio que no aporten información, se implementó un algoritmo para discriminar entre fragmentos correspondientes al habla y segmentos de ruido o silencio, eliminando estos últimos a partir de un análisis de la _Short Time Energy_ (STE) y el _Zero Crossing Rate_ (ZCR). Este procedimiento genera nuevos archivos de audio que contienen únicamente la información acústica relevante.

El procesamiento se realizó mediante la creación de ventanas temporales, dividiendo la señal de entrada en tramos de $20$ ms con un solapamiento del $50\%$ entre ventanas consecutivas. Estos parámetros se seleccionaron para garantizar la cuasi-estacionariedad de la señal de voz, permitiendo un análisis espectral y temporal preciso sin perder continuidad en los bordes de cada tramo.

Para cada ventana se calcularon los dos descriptores mencionados anteriormente empleando `seewave` y `tuneR`:

* **STE**: Se calculó como la suma de los cuadrados de la amplitud de la señal en cada ventana, normalizando en el intervalo $[0,1]$. Esta métrica actúa como el discriminador principal, asumiendo que los segmentos de voz presentan una energía significativamente superior frente a los ruidos de fondo o silencios.

* **ZCR**: Se computó la frecuencia con la que la señal cambia de signo dentro de la ventana. Aunque el código permite su uso como criterio secundario para filtrar ruidos con alta frecuencia, en la configuración final se priorizó el criterio del STE.

El criterio de decisión se estableció mediante un umbral de energía definido. Aquellos tramos con una energía normalizada inferior a $0.02$ ($2\%$ del máximo) se etiquetaron como \emph{Ruido/Silencio}, mientras que las superiores se clasificaron como \emph{Voz/Señal}. Para fijar los umbrales de STE y ZCR se observó los valores típicos normalizados que se obtenían en la mayoría de audios y se fijó a un valor en el que tan solo se perdiera la información menos relevante al eliminar los valores inferiores, dejando tan solo la parte con habla en la señal.

En la siguiente figura se muestra la clasificación para uno de los audios. Los puntos de la STE que superan el umbral corresponden a la señal, mientras que los inferiores corresponden a ruido. 

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\textwidth]{imagenes/ruido.png}
  \caption{Etiquetado de puntos de ruido o señal a partir de la STE normalizada.}
  \label{fig:ruido}
\end{figure}

Finalmente, se reconstruyó la señal de audio concatenando temporalmente las muestras correspondientes a los segmentos validos, descartando los tramos de silencio y exportando el resultado limpio a la carpeta de destino para su posterior caracterización.


```{r, eval=FALSE, include=FALSE}

# ZERO CROSSING RATE
zcr_audio <- function(audio, win_time = 0.02, overlap = 50, plot_result = FALSE){
  fs <- audio@samp.rate
  wlen <- floor(win_time*fs)
  
  zcr_calc <- zcr(audio, wl=wlen, ovlp=overlap, plot=plot_result)
  df_zcr <- data.frame(time = zcr_calc[, 1], zcr_value = zcr_calc[, 2])
  return(df_zcr)
}

# SHORT TIME ENERGY
ste_audio <- function(audio, win_time = 0.02, overlap = 50, plot_result = FALSE){
  signal <- audio@left
  fs <- audio@samp.rate
  N <- length(signal)
  
  overlap_pct <- overlap/100
  wlen <- floor(win_time*fs)
  step_size <- floor(wlen * (1 - overlap_pct))
  num_frames <- floor((N - wlen) / step_size) + 1
  ste_vec <- numeric(num_frames)
  time_axis <- numeric(num_frames)
  
  # Cálculo de la energía
  for (i in 1:num_frames) {
    start_idx <- (i - 1) * step_size + 1
    end_idx <- start_idx + wlen - 1
    
    if(end_idx > N) break
    
    window_data <- signal[start_idx:end_idx]
    
    ste_vec[i] <- sum(window_data^2)
    
    time_axis[i] <- (start_idx + wlen/2) / fs
  }
  
  ste_norm <- (ste_vec - min(ste_vec)) / (max(ste_vec) - min(ste_vec))
  
  if (plot_result) {
    plot(time_axis, ste_norm, type = "l", col = "darkgreen",
         main = paste("STE Normalizada - Ventana:", wlen/fs*1000, "ms"),
         xlab = "Tiempo (s)", ylab = "Energía normalizada")
    grid()
  }
  
  df_ste <- data.frame(time = time_axis, ste = ste_norm)
  return(df_ste)
}
```


```{r,include=FALSE,  eval=FALSE}

# DETECCIÓN RUIDO/SILENCIO

noise_audio <- function(audio, win_time = 0.02, overlap = 50, umbral_zcr = 0.5, umbral_ste = 0.02, plot_result = FALSE){
  datos_ste <- ste_audio(audio, win_time, overlap, plot_result = FALSE)
  datos_zcr <- zcr_audio(audio, win_time, overlap, plot_result = FALSE)
  
  # Sincronizar longitudes
  n_filas <- min(nrow(datos_ste), nrow(datos_zcr))
  datos_ste <- datos_ste[1:n_filas, ]
  datos_zcr <- datos_zcr[1:n_filas, ]
  
  df_final <- datos_ste
  df_final$zcr <- datos_zcr$zcr_value
  df_final$es_ruido <- FALSE
  
  # Criterio 1: baja energía = silencio o ruido de fondo
  df_final$es_ruido[df_final$ste < umbral_ste] <- TRUE
  
  # Criterio 2 (opcional): alta ZCR con energía baja suele ser ruido de viento o siseo
  #df_final$es_ruido[df_final$zcr > umbral_zcr & df_final$ste < 0.2] <- TRUE
  
  df_final$etiqueta <- ifelse(df_final$es_ruido, "Ruido/Silencio", "Voz/Señal")
  
  if (plot_result) {
    plot(df_final$time, df_final$ste, type = "l", col = "gray", lwd = 2,
         main = "Detección Ruido", 
         xlab = "Tiempo (s)", ylab = "Energía Normalizada", ylim = c(0,1))
    
    # Ruido
    points(df_final$time[df_final$es_ruido], 
           df_final$ste[df_final$es_ruido], 
           col = rgb(1, 0, 0, 0.5), pch = 16, cex = 0.5)
    
    # Señal
    points(df_final$time[!df_final$es_ruido], 
           df_final$ste[!df_final$es_ruido], 
           col = rgb(0, 0.6, 0, 0.5), pch = 16, cex = 0.5)
           
    abline(h = umbral_ste, col = "blue", lty = 2, lwd = 2)
    legend("topright", legend = c("Señal", "Ruido", "Umbral"), 
           col = c("darkgreen", "red", "blue"), pch = 16, lty = c(NA, NA, 2))
  }
  
  return(df_final)
}
```

```{r, eval=FALSE, include=FALSE}
# ELIMINACIÓN RUIDO/SILENCIO

clean_audio <- function(audio, df_analisis) {
  fs <- audio@samp.rate
  
  # Tiempos donde NO hay ruido
  tiempos_senal <- df_analisis$time[!df_analisis$es_ruido]

  # Concatenamos solo las muestras que corresponden a tramos de la señal
  dt <- df_analisis$time[2] - df_analisis$time[1]
  muestras_por_trama <- floor(dt * fs)
  
  indices_validos <- which(!df_analisis$es_ruido)
  senal_reconstruida <- numeric()
  
  vec_logico <- rep(FALSE, length(audio@left))
  
  for(idx in indices_validos){
    # Convertimos el tiempo del tramo a índice de muestra aproximado
    centro <- df_analisis$time[idx]
    inicio <- round((centro - dt/2) * fs)
    fin <- round((centro + dt/2) * fs)
    
    # Asegurar límites
    inicio <- max(1, inicio)
    fin <- min(length(audio@left), fin)
    
    vec_logico[inicio:fin] <- TRUE
  }
  
  samples_limpios <- audio@left[vec_logico]
  
  # Creamos un nuevo objeto wave
  wave_limpio <- Wave(left = samples_limpios, samp.rate = fs, bit = audio@bit)
  return(wave_limpio)
}
```

<<<<<<< HEAD
```{r,include=FALSE,  eval=FALSE}
=======
```{r, eval=FALSE, include=FALSE}
>>>>>>> d451dfcf890b59ccd49cf984f1880bccb4ef2b02
# GUARDADO DE AUDIOS LIMPIOS EN UNA NUEVA CARPETA

dir_entrada <- "Audios_finales_conv"
dir_salida <- "Audios_finales_limp"

if (!dir.exists(dir_salida)) {
  dir.create(dir_salida)
}

archivos <- list.files(path = dir_entrada, pattern = "\\.wav$", full.names = TRUE, ignore.case = TRUE)

for (ruta_archivo in archivos) {
  
  nombre_archivo <- basename(ruta_archivo)
  
  tryCatch({
    audio_original <- readWave(ruta_archivo)
    
    resultado_analisis <- noise_audio(audio_original, plot_result = FALSE)
    
    audio_limpio <- clean_audio(audio_original, resultado_analisis)
    
    ruta_guardado <- file.path(dir_salida, nombre_archivo)
      
    writeWave(audio_limpio, ruta_guardado)
  }, error = function(e) {
    message(paste("Error procesando:", nombre_archivo, "-", e$message))
  })
}
```


## EXTRACCIÓN DE CARACTERÍSTICAS

Para el tratamiento de señales de voz en modelos de clasificación, no resulta adecuado trabajar directamente con la señal de audio, ya que estos modelos requieren datos numéricos estructurados. Por este motivo, se lleva a cabo un proceso de extracción de características que permite representar la información relevante de la señal de forma numérica. Dichas características se emplearán posteriormente para el entrenamiento de los modelos.

### Period pitch

El *period pitch* está directamente relacionado con la frecuencia fundamental de la voz, que corresponde a la vibración periódica más baja producida por las cuerdas vocales y constituye la base sobre la que se construyen el resto de componentes armónicos del habla. La frecuencia fundamental es especialmente relevante porque representa una característica estable de la voz y está estrechamente vinculada a la percepción de si una voz suena más grave o más aguda.

Así pues, el period pitch se estimó mediante un método basado en la autocorrelación de la señal, que permite detectar la periodicidad dominante en los segmentos sonoros del audio, principalmente en vocales. A partir del retardo correspondiente al máximo de la autocorrelación se obtiene el período fundamental de la señal, y su inversa proporciona una estimación de la frecuencia fundamental o pitch, expresada en hercios (Hz), es decir, vibraciones por segundo.

De acuerdo con la literatura consultada, los valores típicos de frecuencia fundamental en voz humana se sitúan aproximadamente en el rango comprendido entre 85 y 260 Hz, presentando diferencias claras entre hombres y mujeres (\citep{rabiner1978digital} y \citep{basu2020pitch}). 

A partir de estos rangos reportados, se definieron umbrales prácticos para la clasificación del sexo. Con el fin de evitar clasificaciones forzadas en las zonas de solapamiento entre ambos grupos, se estableció una región intermedia de indeterminación. De este modo, se consideraron voces masculinas aquellas con frecuencias fundamentales inferiores a 170 Hz, voces femeninas aquellas con valores superiores a 180Hz y como indeterminadas aquellas comprendidas entre ambos umbrales.

Para calcular el pitch creamos la función `period_pitch` y, con el fin de garantizar la fiabilidad de la estimación, se incorporan una serie de comprobaciones previas sobre la señal de entrada. En primer lugar, se verifica que el audio contenga un número suficiente de muestras, ya que señales excesivamente cortas no permiten identificar una periodicidad clara. Asimismo, la búsqueda del período fundamental se restringe a un rango de retardos coherente con los valores esperados de frecuencia fundamental en voz humana, evitando así detecciones erróneas asociadas a componentes no relevantes de la señal. En aquellos casos en los que la longitud efectiva de la señal no permite cubrir dicho rango de retardos, el cálculo del pitch se considera no válido. 

Esta decisión se basa en el hecho de que los métodos basados en autocorrelación solo proporcionan estimaciones fiables en segmentos sonoros suficientemente largos y con una periodicidad bien definida, como ocurre principalmente en las partes vocales del habla.


```{r, include=FALSE}
period_pitch <- function(signal, fs = NULL, fmin = 80, fmax = 300) {

  # --- Convertir Wave a vector numérico ---
  if (inherits(signal, "Wave")) {
    if (is.null(fs)) fs <- signal@samp.rate

    if (isTRUE(signal@stereo)) {
      x <- (as.numeric(signal@left) + as.numeric(signal@right)) / 2
    } else {
      x <- as.numeric(signal@left)
    }

    signal <- x
  } else {
    signal <- as.numeric(signal)
  }

  signal <- signal[is.finite(signal)]
  signal <- signal - mean(signal)

  n <- length(signal)
  if (n < 10) stop("La señal tiene muy pocas muestras.")

  # --- Lags según F0 esperado ---
  lag_min <- floor(fs / fmax)
  lag_max <- ceiling(fs / fmin)
  lag_max <- min(lag_max, n - 1)

  if (lag_min >= lag_max) stop("Señal demasiado corta para estimar pitch.")

  ac <- as.numeric(acf(signal, lag.max = lag_max, plot = FALSE)$acf)
  ac <- ac[-1]

  lag <- which.max(ac[lag_min:lag_max]) + lag_min - 1
  period <- lag / fs
  pitch <- 1 / period

  list(
    period = period,
    pitch = pitch,
    fs = fs,
    n = n
  )
}
```

### Coeficientes Mel (MFCC) y representación de la señal

Como parte de la extracción de características espectrales, se calcularon los coeficientes cepstrales en escala Mel (MFCC), ampliamente utilizados en tareas de análisis de voz por su capacidad de capturar información relacionada con el timbre y la envolvente espectral. Los MFCC (coeficientes Mel) son características que resumen el timbre de la voz a partir de su espectro, usando una escala de frecuencias similar a la percepción humana (escala Mel). 

En el código, para calcular estos coeficientes, se usa la función `melfcc()` de la libreria `tuneR`. Esta función divide la señal en ventanas temporales solapadas de duración 25 ms con un salto 10 ms. Para cada ventana se obtiene el espectro (vía FFT), se proyecta sobre un banco de filtros Mel con 40 bandas, se aplica compresión logarítmica y posteriormente una transformación tipo DCT para obtener un número reducido de coeficientes. En este caso se conservan 12 coeficientes por ventana.

Además se calculan $\Delta$ y $\Delta\Delta$ son las derivadas temporales de los MFCC: $\Delta$ mide cómo cambian de un frame al siguiente (dinámica) y $\Delta\Delta$ mide el cambio de ese cambio.



### Centroide espectral

**Centroide espectral global**


Con el objetivo de caracterizar la distribución de la energía en el dominio de la frecuencia, se calculó el centroide espectral global de cada señal de voz. Este descriptor puede interpretarse como el “centro de gravedad” del espectro, ya que corresponde a la media de las frecuencias ponderada por su energía, de modo que valores más elevados indican una mayor concentración de energía en frecuencias altas. Previamente a su cálculo, se eliminó el valor medio de la señal, correspondiente a la componente de continua, ya que su presencia introduce un componente espectral en 0 Hz que no aporta información acústica relevante y puede sesgar artificialmente el centroide hacia bajas frecuencias.

De esta forma, el centroide se obtiene a partir del espectro de potencia calculado mediante la transformada rápida de Fourier, considerando únicamente las componentes de frecuencia positivas. De este modo, cada señal queda representada por un único valor que resume su contenido espectral global.

```{r, echo=FALSE}
spectral_centroid <- function(w) {
  fs <- as.numeric(w@samp.rate)
  x  <- as.numeric(w@left)

  # quitar media para estabilidad
  x <- x - mean(x)

  N <- length(x)

  X <- fft(x)
  K <- floor(N / 2) + 1

  P <- Mod(X[1:K])^2 #potencia espectral
  denom <- sum(P) #denominador del centroide
  if (!is.finite(denom) || denom <= 0) return(0)

  # Evitar overflow: usar double desde el principio
  freqs <- as.numeric(0:(K - 1)) * (fs / as.numeric(N))

  sum(freqs * P) / denom
}
```


**Segmentación temporal en 12 partes**


Con el fin de capturar información espectral a lo largo del tiempo, se desarrolló una segunda función que divide cada señal de audio en doce segmentos temporales consecutivos de igual duración. Para cada uno de estos segmentos se calcula de forma independiente el centroide espectral siguiendo el mismo procedimiento que en el caso global. Esta estrategia permite representar cada audio mediante un vector de doce valores, cada uno correspondiente a un segmento temporal distinto, describiendo así la evolución del centroide espectral a lo largo de la señal. 

La segmentación temporal proporciona una descripción más detallada del contenido espectral, al incorporar información temporal que no queda reflejada en una medida global única.

```{r, echo=FALSE}
spectral_centroid_12 <- function(w, n_parts = 12) {
  fs <- as.numeric(w@samp.rate)
  x  <- as.numeric(w@left)
  x  <- x - mean(x) #evita que se sesgue el centroide a frecuencias bajas

  N <- length(x)
  if (N < n_parts * 10 || fs <= 0) return(rep(NA_real_, n_parts)) #por si el audio es demasiado corto

  seg_len <- floor(N / n_parts) #número de muestras por segmento
  cents <- rep(NA_real_, n_parts)

  for (i in seq_len(n_parts)) {
    start <- (i - 1) * seg_len + 1
    end   <- if (i < n_parts) i * seg_len else N
    seg <- x[start:end]
    seg <- seg - mean(seg)

    L <- length(seg)
    if (L < 2) next #por si el segmento es demasiado corto

    X <- fft(seg)
    K <- floor(L / 2) + 1
    P <- Mod(X[1:K])^2
    denom <- sum(P)
    if (!is.finite(denom) || denom <= 0) next

    freqs <- as.numeric(0:(K - 1)) * (fs / as.numeric(L))
    cents[i] <- sum(freqs * P) / denom
  }

  cents
}
```

## CONSTRUCCIÓN DEL CONJUNTO DE DATOS

### Conjunto de datos general

Para organizar y analizar los 466 audios, se creó un dataframe que contiene las características y etiquetas de cada audio, donde denotamos: 

\begin{itemize}
\item M: masculino / F: femenino
\item P: persona / IA: IA
\item AN: andaluz / AR: argentino / N: neutro
\end{itemize}

Primero, se creó un dataframe que etiquetara cada persona con su sexo, origen y acento, mediante la función `etiquetar_por_nombre()`, que compara el nombre del archivo con los nombres del dataframe de reglas y devuelve las etiquetas correspondientes.

```{r, include = FALSE}
reglas_nombres <- data.frame(
  nombre = c("alvaro", "Oscar", "Iyan", "MAngeles", "aza","Flor","Beatriz","Enrique",
             "Alejandra","Alice","Aurora","Bill","Callum","HermanoFlor","Jessica","Luis","Macarena","Rafael","Tito"),
  sexo   = c("M","M",     "M",    "F",     "F", "F","F","M",
             "F","F","F","M","M","M","F","M","F","M","M"),
  origen = c("P","P","P",    "P",     "P","P","IA","IA",
             "IA","IA","IA","IA","IA","P","IA","IA","IA","IA","IA"),
  acento=c("AN","N","N","AN","N","AR","N","N",
           "N","N","N","N","N","AR","N","N","N","N","N"),
  stringsAsFactors = FALSE
)

reglas_nombres$nombre <- tolower(gsub("\\s+", "", reglas_nombres$nombre))

```

```{r, include = FALSE}
etiquetar_por_nombre <- function(nombre_archivo, reglas) {
  
  for (i in seq_len(nrow(reglas))) {
    if (grepl(reglas$nombre[i], nombre_archivo)) {
      return(list(
        sexo = reglas$sexo[i],
        origen = reglas$origen[i],
        acento=reglas$acento[i]
      ))
    }
  }
  
  return(list(
    sexo = NA,
    origen = NA,
    acento=NA
  ))
}
```


A continuación, para cada audio se incluyeron las siguientes variables: duración, zcr, energía rms, pitch, centroide y centroide en 12 tramos. Para ello se utilizó la función `añadir_voz()` la cuál recibe un archivo de audio, extrae sus características, asigna las etiquetas usando la función anterior y añade toda la información como una nueva fila al dataframe. Así, cada fila del dataframe corresponde a un audio individual, con sus características numéricas y etiquetas.

```{r, include=FALSE}
voces_df <- data.frame(
  señal = character(),
  sexo = character(),
  origen = character(),
  acento = character(),
  duracion = numeric(),
  zcr = numeric(),
  energia_rms = numeric(),
  period_pitch = numeric(),
  centroide = numeric(),
  centroide_1 = numeric(), centroide_2 = numeric(), centroide_3 = numeric(),
  centroide_4 = numeric(), centroide_5 = numeric(), centroide_6 = numeric(),
  centroide_7 = numeric(), centroide_8 = numeric(), centroide_9 = numeric(),
  centroide_10 = numeric(), centroide_11 = numeric(), centroide_12 = numeric(),
  centroide_sd12 = numeric(),
  stringsAsFactors = FALSE
)

añadir_voz <- function(df, ruta_wav) {

  audio <- readWave(ruta_wav)
  fs <- audio@samp.rate
  señal_audio <- audio@left

  duracion <- length(señal_audio) / fs
  zcr_val <- mean(zcr(señal_audio, f = fs, plot = FALSE))
  energia_rms <- sqrt(mean(señal_audio^2))

  nombre_archivo <- tolower(basename(ruta_wav))
  nombre_archivo <- gsub("\\s+", "", nombre_archivo)

  etiquetas <- etiquetar_por_nombre(nombre_archivo, reglas_nombres)
  sexo <- etiquetas$sexo
  origen <- etiquetas$origen
  acento <- etiquetas$acento

  pitch_res <- period_pitch(audio)
  pitch <- pitch_res$pitch

  # Centroide global
  spec_centroid <- spectral_centroid(audio)

  # 12 centroides (vector numérico longitud 12)
  spec_centroid_12 <- spectral_centroid_12(audio)

  # Asegurar longitud 12 (por si acaso) y calcular sd
  if (length(spec_centroid_12) != 12) {
    tmp <- rep(NA_real_, 12)
    tmp[seq_len(min(12, length(spec_centroid_12)))] <- spec_centroid_12[seq_len(min(12, length(spec_centroid_12)))]
    spec_centroid_12 <- tmp
  }
  sd12 <- sd(spec_centroid_12, na.rm = TRUE)

  nueva_fila <- data.frame(
    señal = basename(ruta_wav),
    sexo = sexo,
    origen = origen,
    acento = acento,
    duracion = duracion,
    zcr = zcr_val,
    energia_rms = energia_rms,
    period_pitch = pitch,
    centroide = spec_centroid,
    centroide_1 = spec_centroid_12[1],
    centroide_2 = spec_centroid_12[2],
    centroide_3 = spec_centroid_12[3],
    centroide_4 = spec_centroid_12[4],
    centroide_5 = spec_centroid_12[5],
    centroide_6 = spec_centroid_12[6],
    centroide_7 = spec_centroid_12[7],
    centroide_8 = spec_centroid_12[8],
    centroide_9 = spec_centroid_12[9],
    centroide_10 = spec_centroid_12[10],
    centroide_11 = spec_centroid_12[11],
    centroide_12 = spec_centroid_12[12],
    centroide_sd12 = sd12,
    stringsAsFactors = FALSE
  )

  rbind(df, nueva_fila)
}
```

Cabe destacar que todas las características consideradas en el conjunto de datos están definidas como medias o medidas normalizadas respecto a la longitud de la señal, de modo que no dependen de la duración total del audio. En consecuencia, el proceso de eliminación de silencios únicamente reduce las regiones no informativas de la señal, sin afectar a la coherencia ni a la comparabilidad de las características extraídas entre distintos audios.

Para automatizar la creación del dataframe, se implementó la función `df_carpeta()`, que recorre todos los archivos de la carpeta con los audios limpios y aplica `añadir_voz()` a cada uno. De forma que, se obtiene un  dataframe completo, voces_df, con toda la información lista para análisis y clasificación.

```{r, include=FALSE}
df_carpeta <- function(df, carpeta) {
  archivos <- list.files(
    path = carpeta,
    pattern = "\\.wav$",
    full.names = TRUE
  )
  
  for (ruta in archivos) {
    df <- añadir_voz(df, ruta)
  }
  
  return(df)
}

```

```{r, include=FALSE}
voces_df <- df_carpeta(voces_df, "Audios_finales_limp")
```

### Conjunto de datos para detección de IA: MFCC

Para cada archivo de audio se aplicó un preprocesado común con el objetivo de asegurar la comparabilidad entre señales. En concreto, se convirtió cada audio a mono mediante la función `to_mono()`, promediando los canales izquierdo y derecho en caso de que la señal fuese estéreo. Después, se homogeneizó la frecuencia de muestreo a un valor fijo (16 kHz) mediante la función `resample_if_needed()`, ya que el cálculo de MFCC depende directamente de la escala de frecuencias y no es comparable si los audios están a distintas tasas de muestreo.

```{r,include=FALSE,eval=FALSE}

# calcula un canal mono como el promedio de los canales izquierdo y derecho
to_mono <- function(w) {
  
  if (!is.null(w@stereo) && isTRUE(w@stereo)) {
    mono <- round((w@left + w@right) / 2)
    w <- Wave(left = mono, samp.rate = w@samp.rate, bit = w@bit)
  }
  w
}

#Pone todos los audios en la misma frecuencia de muestreo (por defecto, 16 kHz).
#MFCC depende de la escala de frecuencias. Si un audio está a 48 kHz y otro a 16 kHz, las bandas y la interpretación espectral no son comparables de manera directa.

resample_if_needed <- function(w, target_sr = 16000) {
  # Resampleo a frecuencia fija (recomendado para comparabilidad)
  if (!is.null(target_sr) && w@samp.rate != target_sr) {
    if (!requireNamespace("seewave", quietly = TRUE)) {
      stop("Para resamplear instala 'seewave' o pon target_sr=NULL.")
    }
    # seewave::resamp espera el vector de señal y los rates
    x <- seewave::resamp(w@left, f = w@samp.rate, g = target_sr, output = "sample")
    # reconstruye Wave (manteniendo bit depth)
    w <- Wave(left = as.integer(round(x)), samp.rate = target_sr, bit = w@bit)
    #w <- Wave(left = as.integer(x), samp.rate = target_sr, bit = w@bit)
  }
  w
}

```


Una vez normalizada la señal, se extrajeron los coeficientes MFCC utilizando una configuración fija de ventanas temporales (duración de ventana y salto constantes) y un número determinado de coeficientes cepstrales, para ello se uso la función `melfcc()` como se ha comentado anteriormente. Dado que los MFCC se calculan por frames y generan una matriz temporal, se incorporó además información dinámica calculando las derivadas temporales: ($\Delta$) (delta) y ($\Delta\Delta$) (delta-delta), mediante la función `delta_simple()`, capturando así los cambios espectrales a lo largo del tiempo.

```{r,include=FALSE}
#calcula derivadas temporales (Δ) aproximadas sobre una matriz por frames (MFCC u otra).

#¿Por qué es importante?: Δ y ΔΔ capturan dinámica temporal (cómo cambian los coeficientes), que suele ayudar en clasificación de voz (incluyendo spoof/deepfake).

delta_simple <- function(M) {
  # Delta centrada simple (con división por 2 para aproximar derivada discreta)
  # Maneja audios muy cortos (pocos frames)
  n <- nrow(M)
  if (is.null(n) || n < 2) return(matrix(NA_real_, nrow = n, ncol = ncol(M)))
  if (n == 2) {
    d <- rbind(M[2,] - M[1,], M[2,] - M[1,])
    return(d)
  }
  d <- rbind(
    (M[2, ] - M[1, ]),                          # borde inicial
    (M[3:n, ] - M[1:(n-2), ]) / 2,              # centrada
    (M[n, ] - M[n-1, ])                         # borde final
  )
  d
}
```


Para transformar estas matrices (MFCC, ($\Delta$) y ($\Delta\Delta$)) en un conjunto de variables de tamaño fijo por observación, se resumió cada una mediante estadísticas agregadas. En particular, para cada coeficiente se calcularon su media y su desviación estándar a lo largo de todos los frames del audio. Además, se incluyó un control de calidad eliminando valores no finitos (NaN o infinitos) y descartando los audios excesivamente cortos, con el fin de evitar que la extracción de características generase vectores inestables o no representativos.

Para incorporar toda esta información en el conjunto de datos, se utilizó la función `extract_feat_from_wav()`, la cual recibe un archivo de audio, realiza el preprocesado descrito, calcula MFCC, ($\Delta$) y ($\Delta\Delta$), obtiene los estadísticos (medias y desviaciones), asigna la etiqueta correspondiente y devuelve un vector final de características. Así, cada fila del dataframe corresponde a un audio individual, con sus variables MFCC resumidas y su clase asociada (real o IA), además de metadatos como la ruta del archivo y la frecuencia de muestreo final.

```{r,include=FALSE}
# Transformar un archivo de audio en un único vector de features para meter en un dataframe.
extract_feat_from_wav <- function(
  file,
  label = NA_character_,
  target_sr = 16000,
  wintime = 0.025,
  hoptime = 0.010,
  numcep  = 12,
  nbands  = 40,
  preemph = 0.97,
  dither  = FALSE,
  frames_min = 5
) {
  
  # Lee WAV, calcula MFCC + Δ + ΔΔ y devuelve un vector nombrado (1 fila)
  w <- tryCatch(readWave(file), error = function(e) NULL)
  if (is.null(w)) return(NULL)

  w <- to_mono(w)
  w <- resample_if_needed(w, target_sr = target_sr)

  # melfcc() puede variar según instalación; asumimos como en tu código.
  mfcc <- tryCatch(
    melfcc(
      samples = w,
      wintime = wintime,
      hoptime = hoptime,
      numcep  = numcep,
      nbands  = nbands,
      preemph = preemph,
      dither  = dither
    ),
    error = function(e) NULL
  )
  if (is.null(mfcc)) return(NULL)

  # Evitar clips demasiado cortos
  if (nrow(mfcc) < frames_min) return(NULL)
  
  # Limpieza de no-finitos (Inf/-Inf/NaN) que contaminan colMeans/sd
  mfcc[!is.finite(mfcc)] <- NA_real_

  # Si queda “demasiado roto”, descarta el audio
  if (all(is.na(mfcc))) return(NULL)

  d1 <- delta_simple(mfcc)
  d1[!is.finite(d1)] <- NA_real_

  d2 <- delta_simple(d1)
  d2[!is.finite(d2)] <- NA_real_

  sd_na <- function(x) sd(x, na.rm = TRUE)

  # Estadísticos (1 vector por audio)
  feat <- c(
    colMeans(mfcc, na.rm = TRUE), apply(mfcc, 2, sd_na),
    colMeans(d1,   na.rm = TRUE), apply(d1,   2, sd_na),
    colMeans(d2,   na.rm = TRUE), apply(d2,   2, sd_na)
  )
  
  nm <- c(
    paste0("mfcc_mean_", seq_len(numcep)),
    paste0("mfcc_sd_",   seq_len(numcep)),
    paste0("d1_mean_",   seq_len(numcep)),
    paste0("d1_sd_",     seq_len(numcep)),
    paste0("d2_mean_",   seq_len(numcep)),
    paste0("d2_sd_",     seq_len(numcep))
  )
  names(feat) <- nm

  # Añadimos metadatos
  c(file = normalizePath(file, winslash = "/", mustWork = FALSE),
    label = label,
    sr = as.character(w@samp.rate),
    feat)
  
}

# Recorre carpetas de audios, extrae feat por archivo y devuelve un data.frame final.
build_feat_dataframe <- function(
  real_dir,
  ia_dir,
  pattern = "\\.(wav|WAV)$",
  recursive = TRUE,
  target_sr = 16000,
  ...
) {
  real_files <- list.files(real_dir, pattern = pattern, full.names = TRUE, recursive = recursive)
  ia_files   <- list.files(ia_dir,   pattern = pattern, full.names = TRUE, recursive = recursive)

  files <- c(real_files, ia_files)
  labels <- c(rep("real", length(real_files)), rep("ia", length(ia_files)))

  rows <- vector("list", length(files))
  for (i in seq_along(files)) {
    rows[[i]] <- extract_feat_from_wav(
      file = files[i],
      label = labels[i],
      target_sr = target_sr,
      ...
    )
  }

  # Quitar fallos / audios demasiado cortos
  rows <- rows[!vapply(rows, is.null, logical(1))]
  if (length(rows) == 0) {
    return(data.frame())
  }

  # Pasar a data.frame (todo como character/numeric según corresponda)
  df <- as.data.frame(do.call(rbind, lapply(rows, function(x) {
    # Convertimos a lista para que data.frame no convierta nombres raros
    as.list(x)
  })), stringsAsFactors = FALSE)

  # Convertir columnas numéricas (excepto file/label)
  num_cols <- setdiff(names(df), c("file", "label"))
  df[num_cols] <- lapply(df[num_cols], function(v) suppressWarnings(as.numeric(v)))

  # Reordenar
  df <- df[, c("file", "label", "sr", setdiff(names(df), c("file", "label", "sr")))]
  df
}

```


Para automatizar la creación del dataframe completo, se implementó la función `build_feat_dataframe()`, que recorre todos los archivos _.wav_ de las carpetas real y fake, aplica `extract_feat_from_wav()` a cada uno y une los resultados en un único dataframe.

```{r,include=FALSE, eval=FALSE}
df <- build_feat_dataframe(
  real_dir = "for-norm/training/real",
  ia_dir   = "for-norm/training/fake",
  target_sr = 16000,
  dither = FALSE
)

# Se arregla el dataframe
set.seed(123)

# 1) Target como factor
df <- df %>%
  mutate(label = factor(label, levels = c("real","ia")))

# 2) Eliminar columnas no predictoras
drop_cols <- intersect(names(df), c("file","sr"))
df_ml <- df %>% select(-all_of(drop_cols))

# 3) Quitar filas con NA/Inf (mejor que imputar de inicio)
is_bad <- function(x) is.na(x) | is.infinite(x)
bad_rows <- apply(df_ml %>% select(-label), 1, function(r) any(is_bad(r)))
train_df <- df_ml[!bad_rows, ]
```

## ANÁLISIS EXPLORATORIO

Antes de aplicar los métodos de clasificación, se realizó un análisis exploratorio del conjunto de datos con el objetivo de comprender la distribución de las variables y detectar posibles patrones relevantes. Así, el conjunto de datos final está compuesto por 466 audios, con una distribución equilibrada entre voces femeninas (240) y masculinas (226), lo que reduce la posibilidad de sesgos derivados del desbalanceo de clases.

```{r, include=FALSE}
table(voces_df$sexo)
```

Tras hacer un análisis descriptivo usando `summary()`, se puede ver que la duración de los audios después de limpiarlos y eliminar los silencios presenta una variabilidad moderada, con valores comprendidos aproximadamente entre 1.5 y 5 segundos. Por otro lado, las características ZCR y energía RMS presentan rangos consistentes y valores medios estables, indicando señales con contenido sonoro continuo y sin presencia dominante de ruido. 

```{r, include=FALSE}
summary(voces_df[, c("duracion", "zcr", "energia_rms", "period_pitch","centroide")])
```

En cuanto a la frecuencia fundamental (pitch), se observa un rango amplio de valores (aproximadamente entre 80 y 285 Hz), consistente con voces humanas. El análisis por sexo revela que las voces femeninas presentan valores de pitch más elevados que las masculinas. Si lo representamos usando un diagrama de cajas, podemos ver un solapamiento entre ambas distribuciones, lo que indica que probablemente existan errores de clasificación en algunos audios concretos al emplear únicamente esta característica.

```{r,echo=FALSE,fig.cap="Distribución del pitch según el sexo."}
# Aumentamos el margen derecho
par(mar = c(5, 4, 4, 6))  # c(bottom, left, top, right)

# Boxplot con espacio extra a la derecha
boxplot(period_pitch ~ sexo,
        data = voces_df,
        main = "Distribución del pitch según el sexo",
        ylab = "Pitch (Hz)",
        xlab = "Sexo",
        border = "black",
        xlim = c(0.5, 3.5)) 

# Franjas de color
rect(par("usr")[1], par("usr")[3], par("usr")[2], 170, col = rgb(0,0,1,0.1), border = NA)
rect(par("usr")[1], 170, par("usr")[2], 180, col = rgb(1,1,0,0.2), border = NA)
rect(par("usr")[1], 180, par("usr")[2], par("usr")[4], col = rgb(1,0,0,0.1), border = NA)

# Umbrales
abline(h = 170, lty = 2, lwd = 2)
abline(h = 180,  lty = 2, lwd = 2)

# Imprimimos fuera del plot
par(xpd = TRUE)
legend(x = 3.3, y = max(voces_df$period_pitch),
       legend = c("Hombre", "Indeterminado", "Mujer"),
       fill = c(rgb(0,0,1,0.1), rgb(1,1,0,0.2), rgb(1,0,0,0.1)),
       border = NA,
       bty = "o",
       bg = "white")

par(xpd = FALSE)  # restaurar
```

El análisis de correlación muestra que la duración y la ZCR están fuertemente relacionadas, mientras que el pitch es prácticamente independiente del resto de variables, aportando información complementaria. De manera similar, el centroide espectral muestra baja correlación con las demás características, lo que indica que también contribuye con información adicional útil para la clasificación.

```{r, echo=FALSE}
vars_num <- voces_df[, c("duracion", "zcr", "energia_rms", "period_pitch", "centroide")]
round(cor(vars_num, use = "complete.obs"), 2)
```

En cuanto al centroide espectral global, se observa una diferencia clara entre las voces humanas y las generadas por inteligencia artificial, tanto en el grupo masculino como en el femenino. En ambos casos, las voces generadas por IA presentan valores de centroide más elevados, lo que indica una mayor concentración de energía en frecuencias altas. Esta diferencia es especialmente marcada en las voces masculinas, donde el solapamiento entre clases es reducido. En el caso de las voces femeninas, aunque la tendencia se mantiene, se aprecia un mayor solapamiento entre ambas distribuciones.

```{r,echo=FALSE, fig.cap="Centroide espectral: Humano vs IA por sexo."}
boxplot(centroide ~ origen + sexo,
        data = voces_df,
        main = "Centroide espectral: Humano vs IA (por sexo)",
        xlab = "Sexo y origen",
        ylab = "Centroide espectral (Hz)")

```

Al analizar la desviación típica del centroide espectral obtenida a partir de la segmentación temporal, se esperaba observar una mayor capacidad de separación entre voces humanas y generadas por inteligencia artificial, bajo la hipótesis de que las voces humanas presentan una mayor variabilidad espectral a lo largo del tiempo. Sin embargo, los resultados muestran un solapamiento considerable entre ambas clases, tanto en voces masculinas como femeninas, lo que indica que esta medida agregada, considerada de forma aislada, no resulta suficiente para discriminar de manera fiable entre ambos tipos de voz. Por si solo, los parametros del centroide segmentado no dan mucha información, no obstante, después los analizaremos todos juntos para ver si entre todos ellos en conjunto pueden proporcionar clasificaciones más robustas.

## MODELOS DE CLASIFICACIÓN Y VALIDACIÓN

### Clasificación del sexo

A partir del conjunto de características extraídas y del análisis exploratorio realizado, tenemos como objetivo la clasificación del sexo del hablante. De esta forma, se plantea el problema como una clasificación binaria y se emplea un modelo de regresión logística. Este tipo de modelo permite relacionar las características extraídas con la probabilidad de que una voz corresponda a un hablante masculino o femenino. 

Para ello, se codificó la variable objetivo \texttt{sexo} en formato numérico, asignando el valor 1 a las voces femeninas y 0 a las masculinas. Esta transformación permite el uso de modelos probabilísticos basados en regresión logística.

Inicialmente, se ajustó un modelo de regresión logística multivariante que incluía todas las características disponibles en el conjunto de datos: duración, tasa de cruces por cero (ZCR), energía RMS, frecuencia fundamental (pitch), centroide, así como las variables categóricas de origen y acento. Este primer modelo tiene como objetivo evaluar la contribución conjunta de todas las variables y explorar posibles relaciones entre ellas y la variable respuesta.

```{r, include=FALSE}
voces_df$gender <- ifelse(voces_df$sexo == "F", 1, 0)
table(voces_df$gender)

variables <- c("origen","acento","duracion", "zcr", "energia_rms", "period_pitch", "centroide","gender")
voces_modif <- voces_df[, variables]
voces_modif$origen <- factor(voces_modif$origen)
voces_modif$acento <- factor(voces_modif$acento)

mod <- glm(gender ~ ., data = voces_modif, family = binomial(link="logit"))
summary(mod)

(mod$null.deviance - mod$deviance)/mod$null.deviance
par(mfrow = c(2,2))
```

Al analizar el resumen del modelo usando la función `summary()`, se observó que las únicas variables estadísticamente significativas para la clasificación del sexo eran el period pitch y el centroide, mientras que el resto de variables no aportaban evidencia significativa una vez incorporado dicho descriptor. A partir de esta observación, se decidió contrastar un modelo reducido basado exclusivamente en el pitch y centroide frente al modelo completo mediante un ANOVA, con el fin de comprobar si la complejidad adicional estaba justificada.

```{r, include=FALSE}
mod_simple <- glm(gender ~ period_pitch+centroide, data = voces_modif, family = binomial)
anova(mod_simple, mod, test = "Chisq")
```

El contraste mediante ANOVA entre el modelo reducido (period pitch y centroide) y el modelo completo muestra que la incorporación de estas variables produce una reducción significativa de la devianza. No obstante,vimos en el análisis individual de los coeficientes que el pitch y el centroide aportan información estadísticamente significativa de forma independiente, mientras que el resto de variables no contribuyen de manera relevante. Por ello, se optó por utilizar el modelo reducido para la clasificación del sexo, ya que combina descriptores complementarios, fáciles de calcular a partir de la señal de voz y con menor riesgo de sobreajuste, proporcionando una base robusta y generalizable.

A continuación, se evaluó el desempeño del modelo reducido en un esquema de entrenamiento y prueba. Para ello, se seleccionó aleatoriamente un $70\%$ de las observaciones para ajustar el modelo y se reservó el $30\%$ para validación.

```{r, include=FALSE}
set.seed(512)
train <- sample(nrow(voces_modif),0.7 * nrow (voces_modif))
mod_pred <- glm(gender ~ period_pitch+centroide, family = binomial ( link = logit ) , data =voces_modif[ train ,])
summary ( mod_pred)

(mod_pred$null.deviance - mod_pred$deviance)/mod_pred$null.deviance
```

La bondad de ajuste del modelo se reflejó en un pseudo-R² de 0.727, lo que sugiere que aproximadamente el $72.7\%$  de la variabilidad del conjunto de entrenamiento queda explicada por el modelo basado únicamente en el period pitch y el centroide. Posteriormente, se aplicó el modelo al conjunto de prueba para evaluar su capacidad predictiva.

```{r, include=FALSE}
pred <- predict(mod_pred, voces_modif[-train,], type = "response")
cor(pred, voces_modif$gender[-train])
```

La correlación entre las probabilidades predichas y las clases reales alcanzó un valor de 0.9014, evidenciando una buena concordancia entre las predicciones y las etiquetas de sexo. Veámoslo gráficamente:

```{r, echo=FALSE,fig.cap=" Predicción vs real del sexo."}
plot(pred, voces_modif$gender[-train], pch = 20,
      main = "Predicción vs Real",
      xlab = "Predicción",
      ylab = "Real")
```

Al observar la gráfica de predicciones, se aprecia que la mayoría de los puntos correspondientes a voces masculinas se agrupan cerca de 0, mientras que las voces femeninas se concentran alrededor de 1, lo que indica que el modelo reconoce correctamente la mayoría de los casos. Sin embargo, algunos puntos se sitúan en valores intermedios, mostrando incertidumbre en la predicción.

Sin embargo, en el análisis exploratorio vimos que el diagrama de cajas del pitch tenía zonas donde había solapamiento entre los rangos de frecuencia fundamental de hombres y mujeres. Por lo tanto, es probable que los audios con pitch en el rango intermedio sean precisamente los que generan errores de clasificación, reflejando la variabilidad natural del tono de voz y características individuales que no se capturan únicamente con el period pitch.

En conjunto, estos resultados refuerzan que el pitch es el descriptor más relevante para la clasificación del sexo, pero que la superposición natural de frecuencias humanas introduce un margen de error inevitable, explicando los puntos intermedios en la gráfica de predicciones.

### Clasificación del origen (IA vs Humano): Coeficiente de Mel

Para tratar este problema de clasificación, se probaron diferentes datasets y dataframes, con los que se entrenó un modelo *SVM con kernel RBF*.

Se eligió entrenar un *SVM con kernel RBF* porque es un modelo especialmente adecuado para el tipo de datos que hemos usado.

En nuestro caso, cada audio no se mantiene como una secuencia temporal completa, sino que se transforma en un vector fijo de características: medias y desviaciones estándar de los MFCC y de sus derivadas $\Delta$ y $\Delta\Delta$. Esto convierte el problema en un dataset tabular (una fila por audio y un número moderado de variables, en torno a 72 predictores), similar a un problema clásico de clasificación con variables numéricas.

Con este tipo de variables, es razonable esperar que la separación entre audios reales y audios generados por IA no sea perfectamente lineal. Es decir, las dos clases pueden mezclarse en el espacio de características y necesitar una frontera de decisión más flexible que una simple recta o plano.

Por este motivo se utiliza un SVM con kernel RBF, ya que este modelo:

- Puede capturar relaciones no lineales entre las variables.
- Suele funcionar muy bien cuando hay un número medio de características (ni muy pocas ni miles)\citep{mathworks_fitcsvm}
- Es un baseline clásico y robusto en tareas de voz y audio cuando se trabaja con MFCC y estadísticas agregadas.\citep{temko2006classification}

#### Proceso de entrenamiento

Para entrenar el clasificador se definió un procedimiento de validación y ajuste de hiperparámetros que permite estimar el rendimiento de forma robusta y reducir el riesgo de sobreajuste. 

En primer lugar, se estableció un esquema de validación mediante la función `trainControl()`. En este caso se utilizó validación cruzada repetida (repeated cross-validation), dividiendo el conjunto de entrenamiento en 5 particiones (5 folds), de forma que en cada iteración se entrena el modelo con 4 folds y se valida con el fold restante. Además, el proceso completo se repite repeats = 2 veces. 

Una vez fijado el esquema de validación, se entrenó el modelo mediante la función `train()` (method="svmRadial") del paquete `caret`, obteniendo el modelo entrenado: `svm_fit`.

```{r, include=FALSE,eval=FALSE}

#Definición del esquema de validación
ctrl <- trainControl(
  method = "repeatedcv", #se usa validación cruzada repetida.
  number = 5, #hace 5 folds (5 particiones).
  repeats = 2, #repite el proceso completo de 5 folds 2 veces.
  classProbs = TRUE, #calcula probabilidades de clase (necesario para métricas como ROC)
  summaryFunction = twoClassSummary, #calcula métricas típicas de clasificación binaria (principalmente ROC/AUC)
  savePredictions = "final", #guarda las predicciones del mejor modelo
)

#Entrenamiento del SVM con tuning y preprocesado
svm_fit <- train(
  label ~ ., #fórmula de modelado: label es la variable objetivo y . ->“usar las demás columnas como predictores”.
  data = train_df, #dataset de entrenamiento.
  method = "svmRadial", #entrena un SVM con kernel radial (RBF)
  metric = "ROC", #elige la mejor configuración de hiperparámetros maximizando AUC-ROC.
  trControl = ctrl, #aplica el control de CV definido arriba.
  preProcess = c("center","scale"), #estandariza los predictores.
  tuneLength = 10 #prueba automáticamente una “rejilla”.
)

svm_fit
```

En total se entrenaron 2 modelos, con 2 datasets diferentes: un dataset extraido de internet (enlace al dataset: \href{https://bil.eecs.yorku.ca/datasets/}{dataset for-norm}) y otro dataset formado por nuestros audios y generados con IA.

#### Proceso de testeo

Para evaluar el rendimiento del modelo una vez entrenado, en primer lugar, se generaron las predicciones del modelo sobre el conjunto de test mediante la función `predict()`. Por un lado, se solicitaron las probabilidades por clase, extrayendo concretamente la probabilidad asociada a la clase “IA” y por otro lado, se obtuvo también la predicción final de clase.

Además, se calcularon métricas de clasificación a partir de estas predicciones. En primer lugar, se utilizó `confusionMatrix()` para construir la matriz de confusión comparando las clases predichas y se evaluó el modelo desde un punto de vista probabilístico mediante la curva ROC, empleando la función `roc()` del paquete `pROC`.

```{r,include=FALSE,eval=FALSE}
# 3) Predicción
p <- predict(svm_feat_internet, newdata = df_test_internet_ml, type = "prob")[, "ia"]
pred <- predict(svm_feat_internet, newdata = df_test_internet_ml, type = "raw")

# 4) Métricas
confusionMatrix(pred, df_test_internet_ml$label)

roc_obj <- roc(response = df_test_internet_ml$label, predictor = p, levels = c("real","ia"))
auc(roc_obj)
plot(roc_obj)

# 5) Guardar resultados por archivo
results <- data.frame(
  file = df_test_internet_ok$file,
  label = df_test_internet_ok$label,
  prob_ia = p,
  pred = pred,
  stringsAsFactors = FALSE
)
```

Se realizó el testeo sobre datos del dataset extraido de internet y sobre datos extraidos del dataset creado por nosotros.

#### Centroide espectral

Dado que el centroide espectral mostró diferencias entre voces humanas y generadas por IA, y aunque sus valores aislados no permiten una separación perfecta, resulta interesante evaluar su capacidad discriminativa en conjunto para clasificar el origen de la voz. Para ello, se entrenaron dos modelos de clasificación basados en random forest con el objetivo de evaluar la capacidad discriminativa de las características derivadas del centroide espectral. El primer modelo utiliza exclusivamente el centroide espectral global, mientras que el segundo incorpora los centroides espectrales obtenidos mediante segmentación temporal, junto con su desviación típica. En ambos casos, el análisis se realizó de forma independiente para voces masculinas y femeninas, por lo que se obtuvieron matrices de confusión separadas para cada grupo.

Las matrices de confusión del modelo con el centroide global son las siguientes:

```{r, echo = FALSE}
rf_by_sex <- function(df, sex_value, seed = 123, train_prop = 0.7) {
  d <- subset(df, sexo == sex_value)
  d <- d[is.finite(d$centroide) & !is.na(d$origen), ]
  d$origen <- factor(d$origen)   # niveles: IA / P (o lo que tengas)

  set.seed(seed)
  idx <- sample(nrow(d), size = floor(train_prop * nrow(d)))
  train <- d[idx, ]
  test  <- d[-idx, ]

  rf <- randomForest(origen ~ centroide, data = train)

  pred <- predict(rf, newdata = test)
  conf <- table(Real = test$origen, Pred = pred)

  list(
    sex = sex_value,
    model = rf,
    confusion = conf,
    accuracy = sum(diag(conf)) / sum(conf)
  )
}
```


```{r, echo = FALSE}
rf_M <- rf_by_sex(voces_df, "M")
rf_F <- rf_by_sex(voces_df, "F")

conf_M <- rf_M$confusion
conf_F <- rf_F$confusion

colnames(conf_M) <- paste("Pred_M", colnames(conf_M), sep = "_")
rownames(conf_M) <- paste("Real_M", rownames(conf_M), sep = "_")

colnames(conf_F) <- paste("Pred_F", colnames(conf_F), sep = "_")
rownames(conf_F) <- paste("Real_F", rownames(conf_F), sep = "_")

conf_table <- cbind(conf_M, conf_F)

knitr::kable(conf_table, caption = "Matrices de confusión del modelo basado en el centroide espectral global, separadas por sexo")

```

El modelo de random forest entrenado únicamente con el centroide espectral global presenta una capacidad moderada de clasificación entre voces humanas y generadas por IA. En concreto, se obtiene una precisión del 69,1 % en las voces masculinas y del 77,8 % en las voces femeninas, lo que indica un rendimiento desigual según el sexo. Aunque el centroide global muestra una separación clara en el análisis exploratorio, los resultados del modelo ponen de manifiesto la existencia de errores de clasificación relevantes, especialmente en la identificación de voces humanas etiquetadas como IA. Esto sugiere que, si bien el centroide espectral global captura diferencias estructurales en la distribución espectral de las señales, su uso como única variable explicativa resulta insuficiente para una clasificación robusta, particularmente en presencia de solapamiento entre clases.

```{r, echo = FALSE}
rf_by_sex_centroid12_sd <- function(df, sex_value, seed = 123, train_prop = 0.7) {
  d <- subset(df, sexo == sex_value)

  feature_cols <- c(paste0("centroide_", 1:12), "centroide_sd12")

  # quedarnos con filas válidas
  d <- d[complete.cases(d[, feature_cols]) & !is.na(d$origen), ]
  d$origen <- factor(d$origen)

  set.seed(seed)
  idx <- sample(nrow(d), size = floor(train_prop * nrow(d)))
  train <- d[idx, ]
  test  <- d[-idx, ]

  formula_rf <- as.formula(
    paste("origen ~", paste(feature_cols, collapse = " + "))
  )

  rf <- randomForest(formula_rf, data = train)

  pred <- predict(rf, newdata = test)
  conf <- table(Real = test$origen, Pred = pred)

  list(
    sex = sex_value,
    model = rf,
    confusion = conf,
    accuracy = sum(diag(conf)) / sum(conf)
  )
}
```

```{r, echo = FALSE}
rf12sd_M <- rf_by_sex_centroid12_sd(voces_df, "M") 
rf12sd_F <- rf_by_sex_centroid12_sd(voces_df, "F")

conf_M <- rf12sd_M$confusion
conf_F <- rf12sd_F$confusion

colnames(conf_M) <- paste("Pred_M", colnames(conf_M), sep = "_")
rownames(conf_M) <- paste("Real_M", rownames(conf_M), sep = "_")

colnames(conf_F) <- paste("Pred_F", colnames(conf_F), sep = "_")
rownames(conf_F) <- paste("Real_F", rownames(conf_F), sep = "_")

conf_table_12sd <- cbind(conf_M, conf_F)

knitr::kable(conf_table_12sd, caption = "Matrices de confusión (12 centroides + sd12), separadas por sexo")

```

Al incorporar la información temporal del centroide espectral mediante la segmentación en doce partes, junto con su desviación típica, se observa una mejora consistente en el rendimiento del modelo de clasificación. En particular, la precisión aumenta hasta 77,9 % en las voces masculinas y 81,9 % en las voces femeninas, lo que confirma una ganancia respecto al modelo basado únicamente en el centroide global. A pesar de que ni los centroides segmentados ni la desviación típica presentan una separación clara cuando se analizan de forma individual, su uso conjunto permite al modelo capturar patrones más complejos en la evolución temporal del espectro. Esto se traduce en un aumento de la precisión global, especialmente en la correcta identificación de voces humanas, reduciendo el número de falsos positivos asociados a la clase IA.

En conjunto, los resultados muestran que, aunque el centroide espectral global parece diferenciar de manera más clara entre voces humanas y generadas por IA en un análisis univariado, la utilización conjunta de los centroides espectrales segmentados y su desviación típica ofrece un mejor rendimiento en el contexto de modelos de clasificación multivariantes. Este comportamiento pone de manifiesto que la información relevante no reside únicamente en un descriptor agregado, sino en la combinación de múltiples características que capturan tanto la estructura global como la variabilidad temporal del espectro. Por tanto, el centroide espectral resulta especialmente útil cuando se integra como parte de un conjunto de características más amplio, más que como un descriptor aislado.

Por último, al comparar estos resultados con los obtenidos mediante otros descriptores espectrales más complejos, como los coeficientes cepstrales en escala Mel (MFCC), se observa que estos últimos ya incorporan de forma más eficiente información espectral y temporal relevante para la clasificación. En consecuencia, la inclusión del centroide espectral en modelos que ya utilizan MFCC no aporta una mejora sustancial adicional.

#### Resultados

Se evaluó el rendimiento del sistema bajo tres configuraciones experimentales, combinando distintos conjuntos de entrenamiento y test con el objetivo de analizar tanto el desempeño en condiciones controladas como su capacidad de generalización:

1. Entrenamiento con el dataset de Internet y test con el dataset de Internet, para medir el rendimiento del modelo en el mismo dominio de datos con el que ha sido entrenado.

2. Entrenamiento con el dataset de Internet y test con nuestro dataset, para evaluar la capacidad de generalización del modelo al aplicarlo sobre audios obtenidos en un entorno distinto (cambio de dominio).

3. Entrenamiento con nuestro dataset y test con nuestro dataset, para estimar el rendimiento del modelo cuando se entrena y evalúa específicamente en el contexto y características de nuestro conjunto de datos.

##### Caso 1: Entrenado con dataset de Internet y testeado con dataset de Internet

\[
\begin{array}{c|cc}
\textbf{Predicción} \backslash \textbf{Real} & \textbf{real} & \textbf{ia} \\
\hline
\textbf{real} & 2063 & 106 \\
\textbf{ia}   & 201  & 2264 \\
\end{array}
\]

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\textwidth]{imagenes/int_int_ROC.jpg}
  \caption{Curva ROC del modelo.}
  \label{fig:roc}
\end{figure}

En esta configuración el modelo se evalúa en el mismo dominio en el que fue entrenado. La matriz de confusión muestra un número bajo de errores (106 falsos “real” y 201 falsos “IA”), y se obtiene una `accuracy` \(\approx 0.934\), por lo que esta claro que no es azar.

Además, la curva ROC aparece muy próxima al vértice superior (comportamiento de AUC elevado), lo cual confirma que el modelo separa bien ambas clases cuando el test comparte distribución con el entrenamiento.

Estos resultados son buenos y razonables, y se explican por dos factores principales:

- El dataset de Internet es grande y suele incluir variabilidad suficiente para que el SVM aprenda una frontera no lineal estable en el espacio MFCC (medias/desviaciones + $\Delta$ y $\Delta\Delta$).

- No existe cambio de dominio ya que las condiciones de grabación, tipos de voces de IA y características del preprocesado son similares entre train y test.

##### Caso 2: Entrenado con dataset de Internet y testeado con nuestro dataset

\[
\begin{array}{c|cc}
\textbf{Predicción} \backslash \textbf{Real} & \textbf{real} & \textbf{ia} \\
\hline
\textbf{real} & 177 & 3 \\
\textbf{ia}   & 26  & 105 \\
\end{array}
\]

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\textwidth]{imagenes/int_ntr_ROC.jpg}
  \caption{Curva ROC del modelo.}
  \label{fig:roc}
\end{figure}

Aquí se mide la generalización a un dominio distinto puesto a que son audios grabados por nosotros y audios IA generados también por nosotros. Aun así, el rendimiento sigue siendo alto: `accuracy` \(\approx 0.907\). La curva ROC sigue mostrando una separación clara.

En la matriz de confusión se observa un patrón típico de cambio de dominio:

- Apenas hay falsos “real” cuando el audio es IA (solo 3 casos), es decir, el modelo detecta muy bien IA en este test.

- El principal error es etiquetar como IA algunos audios reales (26 casos), lo que sugiere que ciertas características de nuestros audios reales (grabación, limpieza de silencios, energía residual, etc.) pueden parecerse a rasgos presentes en parte del material sintético del dataset de Internet.

Estos resultados son lógicos: al cambiar de dominio suele aparecer una degradación moderada porque cambian distribuciones (calidad del micrófono, ruido de fondo, compresión, etc.). Aun así, que el rendimiento se mantenga por encima del 90% sugiere que los MFCC agregados y sus deltas están capturando rasgos relativamente transferibles entre datasets.

##### Caso 3: Entrenado con nuestro dataset y testeado con nuestro dataset 

\[
\begin{array}{c|cc}
\textbf{Predicción} \backslash \textbf{Real} & \textbf{real} & \textbf{ia} \\
\hline
\textbf{real} & 153 & 0 \\
\textbf{ia}   & 0  & 108 \\
\end{array}
\]

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\textwidth]{imagenes/ntr_ntr_ROC.jpg}
  \caption{Curva ROC del modelo.}
  \label{fig:roc}
\end{figure}

En esta combinación se obtiene accuracy = 1.0 (sin ningún error) y una ROC perfecta. Este resultado es poco habitual para un escenario realista de detección IA vs humano, y normalmente indica que la clasificación, ha quedado “demasiado fácil” para el modelo.

Esto puede deberse a que el diseño del dataset introduce una estructura muy repetitiva: mismas frases, solo una variedad de 6 personas (por lo que se repiten mucho las mismas voces) y misma cadena de grabación y limpieza, además de realizar el mismo procedimiento de generación IA usando también las mismas frases. Esto puede provocar que el modelo no esté aprendiendo “humanidad vs IA” en un sentido general, sino artefactos muy específicos de nuestro contexto.

En conclusión, los resultados obtenidos no son una evidencia sólida de que el modelo generalice a “cualquier” voz humana vs “cualquier” IA, porque el test probablemente comparte demasiada estructura con el entrenamiento.

### Clasificación del acento

Además de la clasificación por sexo, se intentó abordar la identificación del acento de la voz, considerando tres categorías: castellano neutro, andaluz y argentino. 

Comenzamos haciendo un estudio exploratorio de las características acústicas para comprobar si alguna podía, por sí sola, ayudar a distinguir entre acentos. Nos centramos en la tasa de cruces por cero (ZCR) ya que está muy relacionada con el contenido de altas frecuencias y con la presencia de consonantes fricativas o sonidos sordos, que pueden variar entre acentos, ya sea por mayor aspiración, articulación más “suave” o más “marcada”, etc.

Se representó la distribución del ZCR por acento mediante un diagrama de cajas, del cual se observa que:

Las tres categorías (AN: andaluz, AR: argentino, N: neutro) presentan rangos de ZCR parcialmente solapados, lo que indica que no hay una separación clara solo con esta característica. Sin embargo, el acento neutro (N) muestra una mediana de ZCR ligeramente más alta y una dispersión mayor, mientras que AN y AR parecen algo más compactos y con medianas algo inferiores.

Existen algunos outliers en AR y N (valores de ZCR más altos), que podrían corresponder a audios concretos con más ruido, más fricación o peor calidad de grabación.

En conjunto, el boxplot indica que el ZCR contiene cierta información sobre el acento (las distribuciones no son idénticas), pero la superposición tan grande entre cajas apunta a que no es suficiente por sí solo para discriminar con fiabilidad los acentos. Esto motivó a combinar el ZCR con otras características en un modelo multinomial.

```{r, include=FALSE}
# library(ggplot2)
```

```{r, fig.pos='H', echo=FALSE, fig.cap="Distribución del ZCR por Acento"}
ggplot(voces_modif, aes(x = acento, y = zcr, fill = acento)) + geom_boxplot() + labs(title = "Distribución del ZCR por Acento", x= 'Acento', y = "Zero Crossing Rate") + scale_fill_manual(values = c(
    AN = "#FFE0EC",  
    AR = "#FFB3C6",  
    N  = "#FF6F91"   
  )) + theme_minimal() + theme(plot.title = element_text(hjust = 0.5))
```

Dado lo visto y teniendo en cuenta que se trata de una clasificación multiclase, se planteó un modelo de regresión logística multinomial utilizando como predictores varios descriptores acústicos de bajo nivel: ZCR, la energía RMS, la duración del audio y el pitch period. Estas características se eligieron por su relación con aspectos articulatorios y prosódicos que suelen diferir entre acentos:

La energía RMS resume la intensidad media de la señal y puede reflejar diferencias en el patrón de acentuación o en la “fuerza” con la que se articula. Por otro lado, la duración del audio captura diferencias en el tempo o velocidad de habla, ya que algunos acentos tienden a ser más rápidos o más pausados. Además, el period pitch recoge variaciones en la entonación y altura de la voz, otro rasgo típicamente distintivo entre acentos.

En conjunto, estas variables, permiten modelar tanto aspectos segmentales (contenido en altas frecuencias, presencia de fricativas, etc.) como suprasegmentales (ritmo, intensidad y entonación), que son precisamente los elementos en los que suelen diferir los acentos.

```{r, include=FALSE}
#install.packages("nnet")
library(nnet)

voces_modif$acento <- as.factor(voces_modif$acento)

mod_acento <- multinom(acento ~ zcr + energia_rms + duracion + period_pitch, data = voces_modif, maxit=500)

summary(mod_acento)
```

Para evaluar la capacidad explicativa del modelo se utilizó el pseudo-$R^2$ de McFadden. En regresión logística el $R^2$ clásico no es adecuado porque no existe una descomposición de la varianza análoga a la de la regresión lineal, y por ello se recurre a índices basados en la verosimilitud. El pseudo-$R^2$ de McFadden es uno de los más utilizados porque compara de forma directa el modelo con predictores frente a un modelo nulo, que solo incluye el intercepto. Valores más altos de $R^2_{\text{McFadden}}~$ indican que el modelo con predictores mejora de forma apreciable la explicación de los datos frente al modelo nulo, mientras que valores cercanos a cero sugieren que las variables empleadas apenas añaden capacidad discriminativa entre acentos.

```{r, include=FALSE}
logL_modelo <- logLik(mod_acento)

mod_nulo <- nnet::multinom(acento ~ 1, data = voces_modif)
logL_nulo <- logLik(mod_nulo)

r2_mcfadden <- as.numeric(1 - (logL_modelo / logL_nulo)) #Fórmula de McFadden: 1 - (logL_modelo / logL_nulo)
print(paste("Pseudo R2 (McFadden):", round(r2_mcfadden, 4)))
```

Tras analizar los resultados, vimos que, aunque el modelo consigue ajustar los datos ($R^2_{\text{McFadden}}~$ = 0.5296), los resultados no pueden considerarse fiables. El número de muestras por acento no es equilibrado: el grupo cuenta con tres personas con acento castellano neutro (dos hombres y una mujer), dos con acento andaluz (hombre y mujer) y una sola con acento argentino (mujer). La variabilidad interna de cada clase es muy limitada al usar varios audios de las mismas voces. Esto hace que, en la práctica, el modelo tiende a aprender rasgos de cada persona más que del acento en sí.

Por lo tanto, este análisis no resulta concluyente y se incluye unicamente en el informe como demostración del estudio realizado.

## CONCLUSIONES

El proyecto ha mostrado que es posible analizar señales de voz y extraer información acústica útil para tareas de clasificación. Se han obtenido buenos resultados en la identificación del sexo del hablante y en la diferenciación entre voces humanas y voces generadas por IA, confirmando la eficacia de las técnicas empleadas.

En cambio, la clasificación del acento no ha alcanzado la precisión deseada, principalmente por el tamaño y la diversidad limitada de los datos. Esto señala la necesidad de bases de datos más amplias y variadas y abre una línea clara de mejora futura.

En general, se han cumplido la mayoría de los objetivos y se ha identificado qué descriptores resultan más útiles en cada caso. Además, los resultados muestran que estas técnicas pueden aplicarse a problemas actuales como la detección de voces sintéticas, con posibles implicaciones en seguridad y verificación de identidad. Como continuación del trabajo, sería interesante ampliar la base de datos y explorar modelos más avanzados, tomando estos resultados como punto de partida para seguir profundizando en el análisis automático de la voz.