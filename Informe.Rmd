---
title: "Informe"
author:
- "Azahara Martínez, María de los Ángeles Díaz,Álvaro Nieva, Iyán Álvarez,"
- "Florencia Pellegrini, Óscar Camacho"
date: "2026-01-14"
output:
  pdf_document: default
  html_document: default
bibliography: referencias.bib
link-citations: true
biblio-style: IEEEtran
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  fig.width = 7,   # Ancho global
  fig.height = 5,  # Alto global
  fig.align = "center",
  out.width = "80%" # Esto asegura que en el documento final no ocupen todo el ancho
)
```


```{r, include=FALSE}
library(seewave)
library(tuneR)
library(caret)
library(pROC)
library(dplyr)
```

# OBJETIVOS DEL PROYECTO

El objetivo principal de este trabajo es analizar señales de voz a partir de grabaciones propias y desarrollar modelos de clasificación capaces de identificar distintas características de la señal de audio. Para ello, se parte de un conjunto de audios etiquetados y se emplean técnicas de preprocesado, extracción de características y aprendizaje automático, con el fin de obtener modelos que puedan generalizar su comportamiento ante audios no utilizados durante el entrenamiento.

De manera más específica, los objetivos del trabajo son los siguientes:

* Analizar señales de voz grabadas por los propios autores y prepararlas para su posterior tratamiento mediante técnicas de preprocesado.

* Extraer características acústicas relevantes que permitan describir distintas propiedades de la voz de forma numérica.

* Construir un conjunto de datos estructurado a partir de las características extraídas y las etiquetas conocidas de cada audio.

* Desarrollar y entrenar modelos de clasificación capaces de identificar el sexo del hablante a partir de la señal de voz.

* Desarrollar modelos de clasificación para la identificación del acento, distinguiendo entre español neutro, andaluz y argentino.

* Investigar la viabilidad de diferenciar entre voces humanas y voces generadas por inteligencia artificial a partir de descriptores acústicos.

* Evaluar el rendimiento de los modelos propuestos mediante técnicas de validación adecuadas.


# PREPROCESADO

### Conversión y lectura de audios

Como primer paso del preprocesado, se desarrolló una función para la conversión de archivos de audio desde el formato \texttt{.m4a} al formato \texttt{.wav}, con el objetivo de unificar el tipo de señal y facilitar su posterior análisis. Para ello, se emplearon las librerías \texttt{av} y \texttt{tuneR} de \textsf{R}, que permiten la conversión y lectura de señales de audio de forma eficiente.

```{r}

```

La función implementada convierte cada archivo a una señal monofónica con una frecuencia de muestreo de 16 kHz y guarda los archivos resultantes en un directorio específico. Además, se automatizó el proceso para convertir de manera recursiva todos los archivos \texttt{.m4a} contenidos en una carpeta y sus subdirectorios.

### Detección y eliminación de ruido y silencios

Con el objetivo de mejorar la calidad de la base de datos y eliminar segmentos de audio que no aporten información, se implementó un algoritmo para discriminar entre fragmentos correspondientes al habla y aquellos correspondientes a ruidos o silencios, eliminando estos últimos a partir de un análisis de la _Short Time Energy_ (STE) y el _Zero Crossing Rate_ (ZCR). Este procedimiento genera nuevos archivos de audio que contienen únicamente la información acústica relevante.

El procesamiento se realizó mediante la creación de ventanas temporales, dividiendo la señal de entrada en tramos de $20$ ms con un solapamiento del $50\%$ entre ventanas consecutivas. Estos parámetros se seleccionaron para garantizar la cuasi-estacionariedad de la señal de voz, permitiendo un análisis espectral y temporal preciso sin perder continuidad en los bordes de cada tramo.

Para cada ventana se calcularon los dos descriptores mencionados anteriormente empleando \texttt{seewave} y \texttt{tuneR}:
\begin{itemize}
    \item \textbf{Short Time Energy (STE):} Se calculó como la suma de los cuadrados de la amplitud de la señal en cada ventana, normalizando en el intervalo $[0,1]$. Esta métrica actúa como el discriminador principal, asumiendo que los segmentos de voz presentan una energía significativamente superior frente a los ruidos de fondo o silencios.
    \item \textbf{Zero Crossing Rate (ZCR):} Se computó la frecuencia con la que la señal cambia de signo dentro de la ventana. Aunque el código permite su uso como criterio secundario para filtrar ruidos con alta frecuencia, en la configuración final se priorizó el criterio del STE.
\end{itemize}

El criterio de decisión se estableció mediante un umbral de energía definido. Aquellos tramos con una energía normalizada inferior a $0.02$ ($2\%$ del máximo) se etiquetaron como \emph{Ruido/Silencio}, mientras que las superiores se clasificaron como \emph{Voz/Señal}. Finalmente, se reconstruyó la señal de audio concatenando temporalmente las muestras correspondientes a los segmentos validos, descartando los tramos de silencio y exportando el resultado limpio a la carpeta de destino para su posterior caracterización.

```{r, include=FALSE}
# ZERO CROSSING RATE
zcr_audio <- function(audio, win_time = 0.02, overlap = 50, plot_result = F){
  fs <- audio@samp.rate
  wlen <- floor(win_time*fs)
  
  zcr_calc <- zcr(audio, wl=wlen, ovlp=overlap, plot=plot_result)
  df_zcr <- data.frame(time = zcr_calc[, 1], zcr_value = zcr_calc[, 2])
  return(df_zcr)
}

# SHORT TIME ENERGY
ste_audio <- function(audio, win_time = 0.02, overlap = 50, plot_result = F){
  signal <- audio@left
  fs <- audio@samp.rate
  N <- length(signal)
  
  overlap_pct <- overlap/100
  wlen <- floor(win_time*fs)
  step_size <- floor(wlen * (1 - overlap_pct))
  num_frames <- floor((N - wlen) / step_size) + 1
  ste_vec <- numeric(num_frames)
  time_axis <- numeric(num_frames)
  
  # Cálculo de la energía
  for (i in 1:num_frames) {
    start_idx <- (i - 1) * step_size + 1
    end_idx <- start_idx + wlen - 1
    
    if(end_idx > N) break
    
    window_data <- signal[start_idx:end_idx]
    
    ste_vec[i] <- sum(window_data^2)
    
    time_axis[i] <- (start_idx + wlen/2) / fs
  }
  
  ste_norm <- (ste_vec - min(ste_vec)) / (max(ste_vec) - min(ste_vec))
  
  if (plot_result) {
    plot(time_axis, ste_norm, type = "l", col = "darkgreen",
         main = paste("STE Normalizada - Ventana:", wlen/fs*1000, "ms"),
         xlab = "Tiempo (s)", ylab = "Energía normalizada")
    grid()
  }
  
  df_ste <- data.frame(time = time_axis, ste = ste_norm)
  return(df_ste)
}
```

```{r, include=FALSE}
# DETECCIÓN RUIDO/SILENCIO

noise_audio <- function(audio, win_time = 0.02, overlap = 50, umbral_zcr = 0.05, umbral_ste = 0.02, plot_result = T){
  datos_ste <- ste_audio(audio, win_time, overlap, plot_result = F)
  datos_zcr <- zcr_audio(audio, win_time, overlap, plot_result = F)
  
  # Sincronizar longitudes
  n_filas <- min(nrow(datos_ste), nrow(datos_zcr))
  datos_ste <- datos_ste[1:n_filas, ]
  datos_zcr <- datos_zcr[1:n_filas, ]
  
  df_final <- datos_ste
  df_final$zcr <- datos_zcr$zcr_value
  df_final$es_ruido <- FALSE
  
  # Criterio 1: baja energía = silencio o ruido de fondo
  df_final$es_ruido[df_final$ste < umbral_ste] <- TRUE
  
  # Criterio 2 (opcional): alta ZCR con energía media-baja suele ser ruido de viento o siseo
  #df_final$es_ruido[df_final$zcr > umbral_zcr & df_final$ste < 0.2] <- TRUE
  
  df_final$etiqueta <- ifelse(df_final$es_ruido, "Ruido/Silencio", "Voz/Señal")
  
  if (plot_result) {
    plot(df_final$time, df_final$ste, type = "l", col = "gray", lwd = 2,
         main = "Detección Ruido", 
         xlab = "Tiempo (s)", ylab = "Energía Normalizada", ylim = c(0,1))
    
    # Ruido
    points(df_final$time[df_final$es_ruido], 
           df_final$ste[df_final$es_ruido], 
           col = rgb(1, 0, 0, 0.5), pch = 16, cex = 0.5)
    
    # Señal
    points(df_final$time[!df_final$es_ruido], 
           df_final$ste[!df_final$es_ruido], 
           col = rgb(0, 0.6, 0, 0.5), pch = 16, cex = 0.5)
           
    abline(h = umbral_ste, col = "blue", lty = 2, lwd = 2)
    legend("topright", legend = c("Señal", "Ruido", "Umbral"), 
           col = c("darkgreen", "red", "blue"), pch = 16, lty = c(NA, NA, 2))
  }
  
  return(df_final)
}
```


```{r, include=FALSE}
# ELIMINACIÓN RUIDO/SILENCIO

clean_audio <- function(audio, df_analisis) {
  fs <- audio@samp.rate
  
  # Tiempos donde NO hay ruido
  tiempos_senal <- df_analisis$time[!df_analisis$es_ruido]

  # Concatenamos solo las muestras que corresponden a tramos de la señal
  dt <- df_analisis$time[2] - df_analisis$time[1]
  muestras_por_trama <- floor(dt * fs)
  
  indices_validos <- which(!df_analisis$es_ruido)
  senal_reconstruida <- numeric()
  
  vec_logico <- rep(FALSE, length(audio@left))
  
  for(idx in indices_validos){
    # Convertimos el tiempo del tramo a índice de muestra aproximado
    centro <- df_analisis$time[idx]
    inicio <- round((centro - dt/2) * fs)
    fin <- round((centro + dt/2) * fs)
    
    # Asegurar límites
    inicio <- max(1, inicio)
    fin <- min(length(audio@left), fin)
    
    vec_logico[inicio:fin] <- TRUE
  }
  
  samples_limpios <- audio@left[vec_logico]
  
  # Creamos un nuevo objeto wave
  wave_limpio <- Wave(left = samples_limpios, samp.rate = fs, bit = audio@bit)
  return(wave_limpio)
}
```

# EXTRACCIÓN DE CARACTERÍSTICAS

Las señales de voz contienen mucha información, pero trabajar directamente con el audio no resulta práctico para entrenar modelos de clasificación. Por ello, es necesario extraer una serie de características numéricas que describan la señal de forma más simple y manejable.

La extracción de características permite resumir aspectos importantes de la voz, como su tono, su energía o cómo se distribuye el sonido en distintas frecuencias. De esta manera, cada audio puede representarse mediante un conjunto de valores que capturan sus propiedades principales, evitando depender de la duración del archivo o de pequeñas variaciones que no aportan información relevante.

Estas características sirven como base para entrenar los modelos utilizados en el proyecto, ya que facilitan la comparación entre distintas voces y permiten identificar patrones asociados a diferentes tipos de hablantes o señales. Gracias a este proceso, es posible abordar tareas de clasificación como la identificación del sexo, el acento o la distinción entre voces humanas y generadas por inteligencia artificial.

### Period pitch

El *period pitch* está directamente relacionado con la frecuencia fundamental de la voz, que corresponde a la vibración periódica más baja producida por las cuerdas vocales y constituye la base sobre la que se construyen el resto de componentes armónicos del habla. La frecuencia fundamental es especialmente relevante porque representa una característica estable de la voz y está estrechamente vinculada a la percepción de si una voz suena más grave o más aguda.

El period pitch se estimó mediante un método basado en la autocorrelación de la señal, que permite detectar la periodicidad dominante en los segmentos sonoros del audio, principalmente en vocales. A partir del retardo correspondiente al máximo de la autocorrelación se obtiene el período fundamental de la señal, y su inversa proporciona una estimación de la frecuencia fundamental o pitch, expresada en hercios (Hz), es decir, vibraciones por segundo.

De acuerdo con la literatura consultada, los valores típicos de frecuencia fundamental en voz humana se sitúan aproximadamente en el rango comprendido entre 85 y 260 Hz, presentando diferencias claras entre hombres y mujeres (Rabiner, 1978 y Basu, 2020). 

A partir de estos rangos reportados, se definieron umbrales prácticos para la clasificación del sexo. Con el fin de evitar clasificaciones forzadas en las zonas de solapamiento entre ambos grupos, se estableció una región intermedia de indeterminación. De este modo, se consideraron voces masculinas aquellas con frecuencias fundamentales inferiores a 170 Hz, y voces femeninas aquellas con valores superiores a 180Hz.

Para calcular el pitch creamos la función `period_pitch` y, con el fin de garantizar la fiabilidad de la estimación, se incorporan una serie de comprobaciones previas sobre la señal de entrada. En primer lugar, se verifica que el audio contenga un número suficiente de muestras, ya que señales excesivamente cortas no permiten identificar una periodicidad clara. Asimismo, la búsqueda del período fundamental se restringe a un rango de retardos coherente con los valores esperados de frecuencia fundamental en voz humana, evitando así detecciones erróneas asociadas a componentes no relevantes de la señal. En aquellos casos en los que la longitud efectiva de la señal no permite cubrir dicho rango de retardos, el cálculo del pitch se considera no válido. 

Esta decisión se basa en el hecho de que los métodos basados en autocorrelación solo proporcionan estimaciones fiables en segmentos sonoros suficientemente largos y con una periodicidad bien definida, como ocurre principalmente en las partes vocales del habla.


```{r, include=FALSE}
period_pitch <- function(signal, fs = NULL, fmin = 80, fmax = 300) {

  # --- Convertir Wave a vector numérico ---
  if (inherits(signal, "Wave")) {
    if (is.null(fs)) fs <- signal@samp.rate

    if (isTRUE(signal@stereo)) {
      x <- (as.numeric(signal@left) + as.numeric(signal@right)) / 2
    } else {
      x <- as.numeric(signal@left)
    }

    signal <- x
  } else {
    signal <- as.numeric(signal)
  }

  signal <- signal[is.finite(signal)]
  signal <- signal - mean(signal)

  n <- length(signal)
  if (n < 10) stop("La señal tiene muy pocas muestras.")

  # --- Lags según F0 esperado ---
  lag_min <- floor(fs / fmax)
  lag_max <- ceiling(fs / fmin)
  lag_max <- min(lag_max, n - 1)

  if (lag_min >= lag_max) stop("Señal demasiado corta para estimar pitch.")

  ac <- as.numeric(acf(signal, lag.max = lag_max, plot = FALSE)$acf)
  ac <- ac[-1]

  lag <- which.max(ac[lag_min:lag_max]) + lag_min - 1
  period <- lag / fs
  pitch <- 1 / period

  list(
    period = period,
    pitch = pitch,
    fs = fs,
    n = n
  )
}
```

### Coeficientes Mel (MFCC) y representación de la señal

Como parte de la extracción de características espectrales, se calcularon los coeficientes cepstrales en escala Mel (MFCC), ampliamente utilizados en tareas de análisis de voz por su capacidad de capturar información relacionada con el timbre y la envolvente espectral. Para ello, se aplicó el procedimiento estándar por ventanas cortas: se dividió cada audio en tramas solapadas (ventana de 25 ms y salto de 10 ms), se realizó la transformada rápida de Fourier (FFT) por trama y se obtuvo el espectro de potencia. Posteriormente, dicho espectro se proyectó sobre un banco de filtros en escala Mel (40 bandas), se aplicó una compresión logarítmica y, finalmente, una transformada discreta del coseno (DCT) para obtener un conjunto compacto de coeficientes (12 MFCC por trama). Adicionalmente, se aplicó un pre-énfasis para compensar la caída de energía en altas frecuencias típica de señales de voz.

Con el fin de incorporar información dinámica, se calcularon también las derivadas temporales de primer orden ($\Delta$) y segundo orden ($\Delta\Delta$), que aproximan respectivamente la \emph{velocidad} y la \emph{aceleración} de los MFCC a lo largo del tiempo. 

Para adaptar esta representación por tramas a modelos clásicos de aprendizaje automático, cada audio se transformó en un único vector de características agregando estadísticas globales: media y desviación típica de los MFCC, así como de sus derivadas $\Delta$ y $\Delta\Delta$. De este modo, cada clip queda representado por un vector fijo que resume tanto el contenido espectral promedio como su variabilidad temporal.


### Centroide espectral

#### Centroide espectral global

El centroide espectral global se calculó como una medida resumen de la distribución energética del espectro de cada señal de audio. Este descriptor representa la frecuencia media ponderada por la potencia espectral y está relacionado con la percepción de brillo del sonido. Para su cálculo, se eliminó previamente el componente de continua de la señal con el fin de evitar un sesgo artificial hacia bajas frecuencias. 

El análisis del centroide espectral permitió observar diferencias en los valores medios entre voces humanas y voces generadas por inteligencia artificial, especialmente al separar los resultados por sexo. No obstante, aunque este descriptor mostró cierto potencial discriminativo, su uso de forma aislada resultó limitado debido al solapamiento existente entre clases.

```{r}
spectral_centroid <- function(w) {
  fs <- as.numeric(w@samp.rate)
  x  <- as.numeric(w@left)

  # quitar media para estabilidad
  x <- x - mean(x)

  N <- length(x)

  X <- fft(x)
  K <- floor(N / 2) + 1

  P <- Mod(X[1:K])^2 #potencia espectral
  denom <- sum(P) #denominador del centroide
  if (!is.finite(denom) || denom <= 0) return(0)

  # Evitar overflow: usar double desde el principio
  freqs <- as.numeric(0:(K - 1)) * (fs / as.numeric(N))

  sum(freqs * P) / denom
}
```


#### Segmentación temporal en 12 partes

Con el objetivo de analizar la evolución temporal del centroide espectral y evaluar la estabilidad espectral de las señales, se propuso una segmentación temporal de cada audio en doce partes consecutivas de igual duración. 

Para cada segmento se calculó de forma independiente el centroide espectral, obteniendo así un vector de doce valores por señal. Esta aproximación permite capturar variaciones temporales del contenido espectral que no son visibles al emplear un único centroide global. A partir de estos valores se derivaron medidas estadísticas como la desviación típica y las diferencias entre centroides consecutivos, con el fin de caracterizar la variabilidad espectral de cada voz. 

Aunque estos descriptores no mostraron una separación clara entre voces humanas y sintéticas cuando se analizaron de forma individual, su combinación permitió capturar patrones más complejos que resultaron útiles en modelos de clasificación no lineales.

```{r}
spectral_centroid_12 <- function(w, n_parts = 12) {
  fs <- as.numeric(w@samp.rate)
  x  <- as.numeric(w@left)
  x  <- x - mean(x) #evita que se sesgue el centroide a frecuencias bajas

  N <- length(x)
  if (N < n_parts * 10 || fs <= 0) return(rep(NA_real_, n_parts)) #por si el audio es demasiado corto

  seg_len <- floor(N / n_parts) #número de muestras por segmento
  cents <- rep(NA_real_, n_parts)

  for (i in seq_len(n_parts)) {
    start <- (i - 1) * seg_len + 1
    end   <- if (i < n_parts) i * seg_len else N
    seg <- x[start:end]
    seg <- seg - mean(seg)

    L <- length(seg)
    if (L < 2) next #por si el segmento es demasiado corto

    X <- fft(seg)
    K <- floor(L / 2) + 1
    P <- Mod(X[1:K])^2
    denom <- sum(P)
    if (!is.finite(denom) || denom <= 0) next

    freqs <- as.numeric(0:(K - 1)) * (fs / as.numeric(L))
    cents[i] <- sum(freqs * P) / denom
  }

  cents
}
```

# CONSTRUCCIÓN DEL CONJUNTO DE DATOS

Para organizar y analizar los 466 audios, se creó un dataframe que contiene las características y etiquetas de cada audio, donde denotamos: 

\begin{itemize}
\item M: masculino / F: femenino
\item P: persona / IA: IA
\item AN: andaluz / AR: argentino / N: neutro
\end{itemize}

Primero, se creó un dataframe que etiquetara cada persona con su sexo, origen y acento, mediante la función `etiquetar_por_nombre()`, que compara el nombre del archivo con los nombres del dataframe de reglas y devuelve las etiquetas correspondientes.

```{r, include = FALSE}
reglas_nombres <- data.frame(
  nombre = c("alvaro", "Oscar", "Iyan", "MAngeles", "aza","Flor","Beatriz","Enrique",
             "Alejandra","Alice","Aurora","Bill","Callum","HermanoFlor","Jessica","Luis","Macarena","Rafael","Tito"),
  sexo   = c("M","M",     "M",    "F",     "F", "F","F","M",
             "F","F","F","M","M","M","F","M","F","M","M"),
  origen = c("P","P","P",    "P",     "P","P","IA","IA",
             "IA","IA","IA","IA","IA","P","IA","IA","IA","IA","IA"),
  acento=c("AN","N","N","AN","N","AR","N","N",
           "N","N","N","N","N","AR","N","N","N","N","N"),
  stringsAsFactors = FALSE
)

reglas_nombres$nombre <- tolower(gsub("\\s+", "", reglas_nombres$nombre))

```

```{r, include = FALSE}
etiquetar_por_nombre <- function(nombre_archivo, reglas) {
  
  for (i in seq_len(nrow(reglas))) {
    if (grepl(reglas$nombre[i], nombre_archivo)) {
      return(list(
        sexo = reglas$sexo[i],
        origen = reglas$origen[i],
        acento=reglas$acento[i]
      ))
    }
  }
  
  return(list(
    sexo = NA,
    origen = NA,
    acento=NA
  ))
}
```


A continuación, para cada audio se incluyeron las siguientes variables: duración, zcr, energía rms y pitch. Para ello se utilizó la función `añadir_voz()` la cuál recibe un archivo de audio, extrae sus características, asigna las etiquetas usando la función anterior y añade toda la información como una nueva fila al dataframe. Así, cada fila del dataframe corresponde a un audio individual, con sus características numéricas y etiquetas.

```{r, include=FALSE}
voces_df <- data.frame(
  señal = character(),
  sexo = character(),
  origen = character(),
  acento = character(),
  duracion = numeric(),
  zcr = numeric(),
  energia_rms = numeric(),
  pitch = numeric(),        
  stringsAsFactors = FALSE
)


añadir_voz <- function(df, ruta_wav) {
  
  # Leemos el audio
  audio <- readWave(ruta_wav)
  fs <- audio@samp.rate
  señal_audio <- audio@left
  
  # Duración
  duracion <- length(señal_audio) / fs
  
  # Zero Crossing Rate
 zcr <- mean(zcr(señal_audio, f = fs,plot = F))
  
  # Energía RMS
  energia_rms <- sqrt(mean(señal_audio^2))
  
  # Etiquetado 
  nombre_archivo <- tolower(basename(ruta_wav))
  nombre_archivo <- gsub("\\s+", "", nombre_archivo)

  etiquetas <- etiquetar_por_nombre(nombre_archivo, reglas_nombres)

  sexo <- etiquetas$sexo
  origen <- etiquetas$origen
  acento<-etiquetas$acento
  
  #PERIOD PITCH:
  pitch_res <- period_pitch(audio)
  pitch <- pitch_res$pitch
  
  # Nueva fila
  nueva_fila <- data.frame(
    señal = basename(ruta_wav),
    sexo = sexo,
    origen=origen,
    acento = acento,
    duracion = duracion,
    zcr = zcr,
    energia_rms = energia_rms,
    period_pitch = pitch,
    stringsAsFactors = FALSE
  )
  
  df <- rbind(df, nueva_fila)
  return(df)
}


```

Cabe destacar que todas las características consideradas en el conjunto de datos están definidas como medias o medidas normalizadas respecto a la longitud de la señal, de modo que no dependen de la duración total del audio. En consecuencia, el proceso de eliminación de silencios únicamente reduce las regiones no informativas de la señal, sin afectar a la coherencia ni a la comparabilidad de las características extraídas entre distintos audios.



Para automatizar la creación del dataframe, se implementó la función `df_carpeta()`, que recorre todos los archivos de la carpeta con los audios limpios y aplica `añadir_voz()` a cada uno. De forma que, se obtiene un  dataframe completo, voces_df, con toda la información lista para análisis y clasificación.

```{r, include=FALSE}
df_carpeta <- function(df, carpeta) {
  archivos <- list.files(
    path = carpeta,
    pattern = "\\.wav$",
    full.names = TRUE
  )
  
  for (ruta in archivos) {
    df <- añadir_voz(df, ruta)
  }
  
  return(df)
}

```

```{r, include=FALSE}
voces_df <- df_carpeta(voces_df, "Audios_finales_limp")
```

## Construcción del conjunto de datos para detección de IA: MFCC

Para cada archivo de audio se aplicó un preprocesado común con el objetivo de asegurar la comparabilidad entre señales. En concreto, se convirtió cada audio a mono mediante la función `to_mono()`, promediando los canales izquierdo y derecho en caso de que la señal fuese estéreo. Después, se homogeneizó la frecuencia de muestreo a un valor fijo (16 kHz) mediante la función `resample_if_needed()`, ya que el cálculo de MFCC depende directamente de la escala de frecuencias y no es comparable si los audios están a distintas tasas de muestreo.

```{r,include=FALSE}

# calcula un canal mono como el promedio de los canales izquierdo y derecho
to_mono <- function(w) {
  
  if (!is.null(w@stereo) && isTRUE(w@stereo)) {
    mono <- round((w@left + w@right) / 2)
    w <- Wave(left = mono, samp.rate = w@samp.rate, bit = w@bit)
  }
  w
}

#Pone todos los audios en la misma frecuencia de muestreo (por defecto, 16 kHz).
#MFCC depende de la escala de frecuencias. Si un audio está a 48 kHz y otro a 16 kHz, las bandas y la interpretación espectral no son comparables de manera directa.
resample_if_needed <- function(w, target_sr = 16000) {
  # Resampleo a frecuencia fija (recomendado para comparabilidad)
  if (!is.null(target_sr) && w@samp.rate != target_sr) {
    if (!requireNamespace("seewave", quietly = TRUE)) {
      stop("Para resamplear instala 'seewave' o pon target_sr=NULL.")
    }
    # seewave::resamp espera el vector de señal y los rates
    x <- seewave::resamp(w@left, f = w@samp.rate, g = target_sr, output = "sample")
    # reconstruye Wave (manteniendo bit depth)
    w <- Wave(left = as.integer(round(x)), samp.rate = target_sr, bit = w@bit)
    #w <- Wave(left = as.integer(x), samp.rate = target_sr, bit = w@bit)
  }
  w
}

```


Una vez normalizada la señal, se extrajeron los coeficientes MFCC utilizando una configuración fija de ventanas temporales (duración de ventana y salto constantes) y un número determinado de coeficientes cepstrales, para ello se uso la función `mefcc()` de la librería `tuneR`. Dado que los MFCC se calculan por frames y generan una matriz temporal, se incorporó además información dinámica calculando las derivadas temporales: ($\Delta$) (delta) y ($\Delta\Delta$) (delta-delta), mediante la función `delta_simple()`, capturando así los cambios espectrales a lo largo del tiempo.

```{r,include=FALSE}
#calcula derivadas temporales (Δ) aproximadas sobre una matriz por frames (MFCC u otra).
#Por qué es importante: Δ y ΔΔ capturan dinámica temporal (cómo cambian los coeficientes), que suele ayudar en clasificación de voz (incluyendo spoof/deepfake).
delta_simple <- function(M) {
  # Delta centrada simple (con división por 2 para aproximar derivada discreta)
  # Maneja audios muy cortos (pocos frames)
  n <- nrow(M)
  if (is.null(n) || n < 2) return(matrix(NA_real_, nrow = n, ncol = ncol(M)))
  if (n == 2) {
    d <- rbind(M[2,] - M[1,], M[2,] - M[1,])
    return(d)
  }
  d <- rbind(
    (M[2, ] - M[1, ]),                          # borde inicial
    (M[3:n, ] - M[1:(n-2), ]) / 2,              # centrada
    (M[n, ] - M[n-1, ])                         # borde final
  )
  d
}
```


Para transformar estas matrices (MFCC, ($\Delta$) y ($\Delta\Delta$)) en un conjunto de variables de tamaño fijo por observación, se resumió cada una mediante estadísticas agregadas. En particular, para cada coeficiente se calcularon su media y su desviación estándar a lo largo de todos los frames del audio. Además, se incluyó un control de calidad eliminando valores no finitos (NaN o infinitos) y descartando los audios excesivamente cortos, con el fin de evitar que la extracción de características generase vectores inestables o no representativos.

Para incorporar toda esta información en el conjunto de datos, se utilizó la función `extract_feat_from_wav()`, la cual recibe un archivo de audio, realiza el preprocesado descrito, calcula MFCC, ($\Delta$) y ($\Delta\Delta$), obtiene los estadísticos (medias y desviaciones), asigna la etiqueta correspondiente y devuelve un vector final de características. Así, cada fila del dataframe corresponde a un audio individual, con sus variables MFCC resumidas y su clase asociada (real o IA), además de metadatos como la ruta del archivo y la frecuencia de muestreo final.

```{r,include=FALSE}
# transformar un archivo de audio en un único vector de features para meter en un dataframe.
extract_feat_from_wav <- function(
  file,
  label = NA_character_,
  target_sr = 16000,
  wintime = 0.025,
  hoptime = 0.010,
  numcep  = 12,
  nbands  = 40,
  preemph = 0.97,
  dither  = FALSE,
  frames_min = 5
) {
  # Lee WAV, calcula MFCC + Δ + ΔΔ y devuelve un vector nombrado (1 fila)
  w <- tryCatch(readWave(file), error = function(e) NULL)
  if (is.null(w)) return(NULL)

  w <- to_mono(w)
  w <- resample_if_needed(w, target_sr = target_sr)

  # melfcc() puede variar según instalación; asumimos como en tu código.
  mfcc <- tryCatch(
    melfcc(
      samples = w,
      wintime = wintime,
      hoptime = hoptime,
      numcep  = numcep,
      nbands  = nbands,
      preemph = preemph,
      dither  = dither
    ),
    error = function(e) NULL
  )
  if (is.null(mfcc)) return(NULL)

  # Evitar clips demasiado cortos
  if (nrow(mfcc) < frames_min) return(NULL)
  
  
  # Limpieza de no-finitos (Inf/-Inf/NaN) que contaminan colMeans/sd
  mfcc[!is.finite(mfcc)] <- NA_real_

  # Si queda “demasiado roto”, descarta el audio
  if (all(is.na(mfcc))) return(NULL)

  d1 <- delta_simple(mfcc)
  d1[!is.finite(d1)] <- NA_real_

  d2 <- delta_simple(d1)
  d2[!is.finite(d2)] <- NA_real_

  sd_na <- function(x) sd(x, na.rm = TRUE)

  # Estadísticos (1 vector por audio)
  feat <- c(
    colMeans(mfcc, na.rm = TRUE), apply(mfcc, 2, sd_na),
    colMeans(d1,   na.rm = TRUE), apply(d1,   2, sd_na),
    colMeans(d2,   na.rm = TRUE), apply(d2,   2, sd_na)
  )
  
  nm <- c(
    paste0("mfcc_mean_", seq_len(numcep)),
    paste0("mfcc_sd_",   seq_len(numcep)),
    paste0("d1_mean_",   seq_len(numcep)),
    paste0("d1_sd_",     seq_len(numcep)),
    paste0("d2_mean_",   seq_len(numcep)),
    paste0("d2_sd_",     seq_len(numcep))
  )
  names(feat) <- nm

  # Añadimos metadatos
  c(file = normalizePath(file, winslash = "/", mustWork = FALSE),
    label = label,
    sr = as.character(w@samp.rate),
    feat)
  
}

# Recorre carpetas de audios, extrae feat por archivo y devuelve un data.frame final.
build_feat_dataframe <- function(
  real_dir,
  ia_dir,
  pattern = "\\.(wav|WAV)$",
  recursive = TRUE,
  target_sr = 16000,
  ...
) {
  real_files <- list.files(real_dir, pattern = pattern, full.names = TRUE, recursive = recursive)
  ia_files   <- list.files(ia_dir,   pattern = pattern, full.names = TRUE, recursive = recursive)

  files <- c(real_files, ia_files)
  labels <- c(rep("real", length(real_files)), rep("ia", length(ia_files)))

  rows <- vector("list", length(files))
  for (i in seq_along(files)) {
    rows[[i]] <- extract_feat_from_wav(
      file = files[i],
      label = labels[i],
      target_sr = target_sr,
      ...
    )
  }

  # Quitar fallos / audios demasiado cortos
  rows <- rows[!vapply(rows, is.null, logical(1))]
  if (length(rows) == 0) {
    return(data.frame())
  }

  # Pasar a data.frame (todo como character/numeric según corresponda)
  df <- as.data.frame(do.call(rbind, lapply(rows, function(x) {
    # Convertimos a lista para que data.frame no convierta nombres raros
    as.list(x)
  })), stringsAsFactors = FALSE)

  # Convertir columnas numéricas (excepto file/label)
  num_cols <- setdiff(names(df), c("file", "label"))
  df[num_cols] <- lapply(df[num_cols], function(v) suppressWarnings(as.numeric(v)))

  # Reordenar
  df <- df[, c("file", "label", "sr", setdiff(names(df), c("file", "label", "sr")))]
  df
}

```


Para automatizar la creación del dataframe completo, se implementó la función `build_feat_dataframe()`, que recorre todos los archivos .wav de las carpetas real y fake, aplica `extract_feat_from_wav()` a cada uno y une los resultados en un único dataframe.

```{r,include=FALSE, eval=FALSE}
df <- build_feat_dataframe(
  real_dir = "for-norm/training/real",
  ia_dir   = "for-norm/training/fake",
  target_sr = 16000,
  dither = FALSE
)

# Se arregla el dataframe
set.seed(123)

# 1) Target como factor
df <- df %>%
  mutate(label = factor(label, levels = c("real","ia")))

# 2) Eliminar columnas no predictoras
drop_cols <- intersect(names(df), c("file","sr"))
df_ml <- df %>% select(-all_of(drop_cols))

# 3) Quitar filas con NA/Inf (mejor que imputar de inicio)
is_bad <- function(x) is.na(x) | is.infinite(x)
bad_rows <- apply(df_ml %>% select(-label), 1, function(r) any(is_bad(r)))
train_df <- df_ml[!bad_rows, ]
```



# ANÁLISIS EXPLORATORIO

Antes de aplicar los métodos de clasificación, se realizó un análisis exploratorio del conjunto de datos con el objetivo de comprender la distribución de las variables y detectar posibles patrones relevantes. Así, el conjunto de datos final está compuesto por 466 audios, con una distribución equilibrada entre voces femeninas (240) y masculinas (226), lo que reduce la posibilidad de sesgos derivados del desbalanceo de clases.

```{r, include=FALSE}
table(voces_df$sexo)
```

Tras hacer un análisis descriptivo usando `summary()`, se puede ver que la duración de los audios después de limpiarlos y eliminar los silencios presenta una variabilidad moderada, con valores comprendidos aproximadamente entre 1.5 y 5 segundos. Por otro lado, las características ZCR y energía RMS presentan rangos consistentes y valores medios estables, indicando señales con contenido sonoro continuo y sin presencia dominante de ruido. 

```{r, include=FALSE}
summary(voces_df[, c("duracion", "zcr", "energia_rms", "period_pitch")])
```

En cuanto a la frecuencia fundamental (pitch), se observa un rango amplio de valores (aproximadamente entre 80 y 285 Hz), consistente con voces humanas. El análisis por sexo revela que las voces femeninas presentan valores de pitch más elevados que las masculinas. Si lo representamos usando un diagrama de cajas, podemos ver un solapamiento entre ambas distribuciones, lo que indica que probablemente existan errores de clasificación en algunos audios concretos al emplear únicamente esta característica.

```{r, echo=FALSE}
boxplot(period_pitch ~ sexo,
        data = voces_df,
        main = "Distribución del pitch según el sexo",
        ylab = "Pitch (Hz)",
        xlab = "Sexo")
```

Por último, el análisis de correlación entre las variables numéricas muestra una alta correlación entre la duración y la ZCR, mientras que el pitch presenta una correlación prácticamente nula con el resto de variables. Esto sugiere que el pitch aporta información complementaria y relevante para la tarea de clasificación.

```{r, echo=FALSE}
vars_num <- voces_df[, c("duracion", "zcr", "energia_rms", "period_pitch")]
round(cor(vars_num, use = "complete.obs"), 2)
```


# MODELOS DE CLASIFICACIÓN Y VALIDACIÓN

## Clasificación del sexo

A partir del conjunto de características extraídas y del análisis exploratorio realizado, tenemos como objetivo la clasificación del sexo del hablante. De esta forma, se plantea el problema como una clasificación binaria y se emplea un modelo de regresión logística. Este tipo de modelo permite relacionar las características extraídas con la probabilidad de que una voz corresponda a un hablante masculino o femenino. 

Para ello, se codificó la variable objetivo \texttt{sexo} en formato numérico, asignando el valor 1 a las voces femeninas y 0 a las masculinas. Esta transformación permite el uso de modelos probabilísticos basados en regresión logística.

Inicialmente, se ajustó un modelo de regresión logística multivariante que incluía todas las características disponibles en el conjunto de datos: duración, tasa de cruces por cero (ZCR), energía RMS, frecuencia fundamental (pitch), así como las variables categóricas de origen y acento. Este primer modelo tiene como objetivo evaluar la contribución conjunta de todas las variables y explorar posibles relaciones entre ellas y la variable respuesta.

```{r, include=FALSE}
voces_df$gender <- ifelse(voces_df$sexo == "F", 1, 0)
table(voces_df$gender)

voces_modif<-voces_df[-1]
voces_modif$origen <- factor(voces_modif$origen)
voces_modif$acento <- factor(voces_modif$acento)

mod <- glm(gender ~ ., data = voces_modif, family = binomial(link="logit"))
summary(mod)

(mod$null.deviance - mod$deviance)/mod$null.deviance
par(mfrow = c(2,2))
```

Al analizar el resumen del modelo de regresión logística completo mediante la función `summary()`, se observó que la única variable estadísticamente significativa para la clasificación del sexo era la frecuencia fundamental (period pitch), mientras que el resto de variables no aportaban evidencia significativa una vez incorporado dicho descriptor. A partir de esta observación, se decidió contrastar un modelo reducido basado exclusivamente en el pitch frente al modelo completo mediante un ANOVA, con el fin de comprobar si la complejidad adicional estaba justificada.

```{r, echo=FALSE}
mod_simple <- glm(gender ~ period_pitch, data = voces_modif, family = binomial)
anova(mod_simple, mod, test = "Chisq")
```
El contraste mediante ANOVA entre el modelo reducido y el modelo completo muestra que incorporar el resto de variables produce una reducción significativa de la devianza (p < 0.001). No obstante, el análisis individual de los coeficientes revela que únicamente el period pitch resulta estadísticamente significativo, mientras que el resto de variables no aportan información relevante de forma independiente.


A continuación, se evaluó el desempeño del modelo reducido en un esquema de entrenamiento y prueba. Para ello, se seleccionó aleatoriamente un $70\%$ de las observaciones para ajustar el modelo y se reservó el $30\%$ para validación.

```{r, include=FALSE}
set.seed(0)
train <- sample(nrow(voces_modif),0.7 * nrow (voces_modif))
mod_pred <- glm(gender ~ period_pitch, family = binomial ( link = logit ) , data =voces_modif[ train ,])
summary ( mod_pred)

(mod_pred$null.deviance - mod_pred$deviance)/mod_pred$null.deviance
```

La bondad de ajuste del modelo se reflejó en un pseudo-R² de 0.697, lo que sugiere que aproximadamente el $69.7\%$  de la variabilidad del conjunto de entrenamiento queda explicada por el modelo basado únicamente en el period pitch. Posteriormente, se aplicó el modelo al conjunto de prueba para evaluar su capacidad predictiva.

```{r, include=FALSE}
pred <- predict(mod_pred, voces_modif[-train,], type = "response")
cor(pred, voces_modif$gender[-train])
```

La correlación entre las probabilidades predichas y las clases reales alcanzó un valor de 0.918, evidenciando una buena concordancia entre las predicciones y las etiquetas de sexo. 

```{r, echo=FALSE}
plot(pred, voces_modif$gender[-train], pch = 20)
```

Al observar la gráfica de predicciones, se aprecia que la mayoría de los puntos correspondientes a voces masculinas se agrupan cerca de 0, mientras que las voces femeninas se concentran alrededor de 1, lo que indica que el modelo reconoce correctamente la mayoría de los casos. Sin embargo, algunos puntos se sitúan en valores intermedios, mostrando incertidumbre en la predicción.

Sin embargo, en el análisis exploratorio vimos que el diagrama de cajas del pitch tenía zonas donde había solapamiento entre los rangos de frecuencia fundamental de hombres y mujeres. Por lo tanto, es probable que los audios con pitch en el rango intermedio sean precisamente los que generan errores de clasificación, reflejando la variabilidad natural del tono de voz y características individuales que no se capturan únicamente con el period pitch.

En conjunto, estos resultados refuerzan que el pitch es el descriptor más relevante para la clasificación del sexo, pero que la superposición natural de frecuencias humanas introduce un margen de error inevitable, explicando los puntos intermedios en la gráfica de predicciones.


## Clasificación del acento

Además de la clasificación por sexo, se intentó abordar la identificación del acento de la voz, considerando tres categorías: español neutro, andaluz y argentino. 
Para esta tarea se planteó el problema como una clasificación multiclase, por lo que realizamos un modelo de regresión logística multinomial, utilizando como variables predictoras descriptores acústicos de bajo nivel, concretamente la tasa de cruces por cero (ZCR), la energía RMS, la duración del audio y el pitch period. Estas características se seleccionaron por su relación con aspectos articulatorios y prosódicos que pueden variar entre acentos.

Con el fin de explorar la separabilidad entre clases, se realizó un análisis gráfico de la distribución del ZCR en función del acento, observándose diferencias apreciables entre algunas categorías. Posteriormente, se ajustó el modelo multinomial y se evaluó su capacidad explicativa mediante el pseudo-$R^2$ de McFadden, comparando la verosimilitud del modelo completo con la de un modelo nulo.

Aunque el modelo consigue ajustar los datos disponibles, los resultados no pueden considerarse fiables: el número de muestras por acento no es equitativo dado que en el grupo hay 3 personas que graban audios con acentro español neutro (2 hombres y una mujer), 2 personas que graban audios con acento andaluz (hombre y mujer), y una sola persona con acento argentino (mujer). Es por tanto que la variabilidad interna de cada clase es mínima (varios audios de las mismas voces) y la evaluación se realiza sobre los mismos registros usados para el entrenamiento. En la práctica, el modelo tiende a aprender rasgos de cada locutor concreto más que del acento en sí. Por este motivo, este análisis se interpreta únicamente como una prueba de concepto del flujo de trabajo (extracción de características, ajuste y evaluación de un modelo multinomial), y se concluye que sería necesario un conjunto de datos mucho más amplio, equilibrado y diverso para poder abordar de forma realista la clasificación automática del acento.


## Clasificación del origen (IA vs Humano)

Para tratar este problema de clasificación, se probaron diferentes datasets y dataframes, con los que se entrenó un modelo *SVM con kernel RBF*.

Se eligió entrenar un *SVM con kernel RBF* porque es un modelo especialmente adecuado para el tipo de datos que hemos usado.

En nuestro caso, cada audio no se mantiene como una secuencia temporal completa, sino que se transforma en un vector fijo de características: medias y desviaciones estándar de los MFCC y de sus derivadas ($\Delta$) y ($\Delta\Delta$). Esto convierte el problema en un dataset tabular (una fila por audio y un número moderado de variables, en torno a 72 predictores), similar a un problema clásico de clasificación con variables numéricas.

Con este tipo de variables, es razonable esperar que la separación entre audios reales y audios generados por IA no sea perfectamente lineal. Es decir, las dos clases pueden mezclarse en el espacio de características y necesitar una frontera de decisión más flexible que una simple recta o plano.

Por este motivo se utiliza un SVM con kernel RBF, ya que este modelo:

- Puede capturar relaciones no lineales entre las variables.
- Suele funcionar muy bien cuando hay un número medio de características (ni muy pocas ni miles)
- Es un baseline clásico y robusto en tareas de voz y audio cuando se trabaja con MFCC y estadísticas agregadas. [@temko2006classification]

### Proceso de entrenamiento

Para entrenar el clasificador se definió un procedimiento de validación y ajuste de hiperparámetros que permite estimar el rendimiento de forma robusta y reducir el riesgo de sobreajuste. 

En primer lugar, se estableció un esquema de validación mediante la función trainControl(). En este caso se utilizó validación cruzada repetida (repeated cross-validation), dividiendo el conjunto de entrenamiento en 5 particiones (5 folds), de forma que en cada iteración se entrena el modelo con 4 folds y se valida con el fold restante. Además, el proceso completo se repite repeats = 2 veces. 
`
Una vez fijado el esquema de validación, se entrenó el modelo mediante la función `train()` (method="svmRadial") del paquete `caret`, obteniendo el modelo entrenado: `svm_fit`.

```{r, include=FALSE,eval=FALSE}

#Definición del esquema de validación
ctrl <- trainControl(
  method = "repeatedcv", #se usa validación cruzada repetida.
  number = 5, #hace 5 folds (5 particiones).
  repeats = 2, #repite el proceso completo de 5 folds 2 veces.
  classProbs = TRUE, #calcula probabilidades de clase (necesario para métricas como ROC)
  summaryFunction = twoClassSummary, #calcula métricas típicas de clasificación binaria (principalmente ROC/AUC)
  savePredictions = "final", #guarda las predicciones del mejor modelo
)

#Entrenamiento del SVM con tuning y preprocesado
svm_fit <- train(
  label ~ ., #fórmula de modelado: label es la variable objetivo y . ->“usar las demás columnas como predictores”.
  data = train_df, #dataset de entrenamiento.
  method = "svmRadial", #entrena un SVM con kernel radial (RBF)
  metric = "ROC", #elige la mejor configuración de hiperparámetros maximizando AUC-ROC.
  trControl = ctrl, #aplica el control de CV definido arriba.
  preProcess = c("center","scale"), #estandariza los predictores.
  tuneLength = 10 #prueba automáticamente una “rejilla”.
)

svm_fit
```

En total se entrenaron 2 modelos, con 2 datasets diferentes: un dataset extraido de internet y otro dataset formado por audios nuestros y generados con IA por nosotros.

### Proceso de testeo

Para evaluar el rendimiento del modelo una vez entrenado, en primer lugar, se generaron las predicciones del modelo sobre el conjunto de test mediante la función `predict()`. Por un lado, se solicitaron las probabilidades por clase, extrayendo concretamente la probabilidad asociada a la clase “ia” y por otro lado, se obtuvo también la predicción final de clase.

Además, se calcularon métricas de clasificación a partir de estas predicciones. En primer lugar, se utilizó `confusionMatrix()` para construir la matriz de confusión comparando las clases predichas y se evaluó el modelo desde un punto de vista probabilístico mediante la curva ROC, empleando la función `roc()` del paquete `pROC`.

```{r,include=FALSE,eval=FALSE}
# 3) Predicción
p <- predict(svm_feat_internet, newdata = df_test_internet_ml, type = "prob")[, "ia"]
pred <- predict(svm_feat_internet, newdata = df_test_internet_ml, type = "raw")

# 4) Métricas
confusionMatrix(pred, df_test_internet_ml$label)

roc_obj <- roc(response = df_test_internet_ml$label, predictor = p, levels = c("real","ia"))
auc(roc_obj)
plot(roc_obj)

# 5) Guardar resultados por archivo
results <- data.frame(
  file = df_test_internet_ok$file,
  label = df_test_internet_ok$label,
  prob_ia = p,
  pred = pred,
  stringsAsFactors = FALSE
)
```

Se realizó el testeo sobre datos del dataset extraido de internet y sobre datos extraidos del dataset creado por nosotros.

### Resultados

Se evaluó el rendimiento del sistema bajo tres configuraciones experimentales, combinando distintos conjuntos de entrenamiento y test con el objetivo de analizar tanto el desempeño en condiciones controladas como su capacidad de generalización:

1. Entrenamiento con el dataset de Internet y test con el dataset de Internet, para medir el rendimiento del modelo en el mismo dominio de datos con el que ha sido entrenado.

2. Entrenamiento con el dataset de Internet y test con nuestro dataset, para evaluar la capacidad de generalización del modelo al aplicarlo sobre audios obtenidos en un entorno distinto (cambio de dominio).

3. Entrenamiento con nuestro dataset y test con nuestro dataset, para estimar el rendimiento del modelo cuando se entrena y evalúa específicamente en el contexto y características de nuestro conjunto de datos.

 (COMPLETAR RESULTADOS)

# REFERENCIAS