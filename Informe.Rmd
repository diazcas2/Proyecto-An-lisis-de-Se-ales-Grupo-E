---
title: "Informe"
author:
- "Azahara Martínez, María de los Ángeles Díaz,Álvaro Nieva, Iyán Álvarez,"
- "Florencia Pellegrini, Óscar Camacho"
date: "2026-01-14"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  fig.width = 7,   # Ancho global
  fig.height = 5,  # Alto global
  fig.align = "center",
  out.width = "80%" # Esto asegura que en el documento final no ocupen todo el ancho
)
```


```{r, include=FALSE}
library(seewave)
library(tuneR)
```

# OBJETIVOS DEL PROYECTO

El objetivo principal de este trabajo es analizar señales de voz a partir de grabaciones propias y desarrollar modelos de clasificación capaces de identificar distintas características de la señal de audio. Para ello, se parte de un conjunto de audios etiquetados y se emplean técnicas de preprocesado, extracción de características y aprendizaje automático, con el fin de obtener modelos que puedan generalizar su comportamiento ante audios no utilizados durante el entrenamiento.

De manera más específica, los objetivos del trabajo son los siguientes:

* Analizar señales de voz grabadas por los propios autores y prepararlas para su posterior tratamiento mediante técnicas de preprocesado.

* Extraer características acústicas relevantes que permitan describir distintas propiedades de la voz de forma numérica.

* Construir un conjunto de datos estructurado a partir de las características extraídas y las etiquetas conocidas de cada audio.

* Desarrollar y entrenar modelos de clasificación capaces de identificar el sexo del hablante a partir de la señal de voz.

* Desarrollar modelos de clasificación para la identificación del acento, distinguiendo entre español neutro, andaluz y argentino.

* Investigar la viabilidad de diferenciar entre voces humanas y voces generadas por inteligencia artificial a partir de descriptores acústicos.

* Evaluar el rendimiento de los modelos propuestos mediante técnicas de validación adecuadas.


# PREPROCESADO

### Conversión y lectura de audios

Como primer paso del preprocesado, se desarrolló una función para la conversión de archivos de audio desde el formato \texttt{.m4a} al formato \texttt{.wav}, con el objetivo de unificar el tipo de señal y facilitar su posterior análisis. Para ello, se emplearon las librerías \texttt{av} y \texttt{tuneR} de \textsf{R}, que permiten la conversión y lectura de señales de audio de forma eficiente.

```{r}

```

La función implementada convierte cada archivo a una señal monofónica con una frecuencia de muestreo de 16 kHz y guarda los archivos resultantes en un directorio específico. Además, se automatizó el proceso para convertir de manera recursiva todos los archivos \texttt{.m4a} contenidos en una carpeta y sus subdirectorios.

### Detección y eliminación de ruido y silencios

Con el objetivo de mejorar la calidad de la base de datos y eliminar segmentos de audio que no aporten información, se implementó un algoritmo para discriminar entre fragmentos correspondientes al habla y aquellos correspondientes a ruidos o silencios, eliminando estos últimos a partir de un análisis de la _Short Time Energy_ (STE) y el _Zero Crossing Rate_ (ZCR). Este procedimiento genera nuevos archivos de audio que contienen únicamente la información acústica relevante.

El procesamiento se realizó mediante la creación de ventanas temporales, dividiendo la señal de entrada en tramos de $20$ ms con un solapamiento del $50\%$ entre ventanas consecutivas. Estos parámetros se seleccionaron para garantizar la cuasi-estacionariedad de la señal de voz, permitiendo un análisis espectral y temporal preciso sin perder continuidad en los bordes de cada tramo.

Para cada ventana se calcularon los dos descriptores mencionados anteriormente empleando \texttt{seewave} y \texttt{tuneR}:
\begin{itemize}
    \item \textbf{Short Time Energy (STE):} Se calculó como la suma de los cuadrados de la amplitud de la señal en cada ventana, normalizando en el intervalo $[0,1]$. Esta métrica actúa como el discriminador principal, asumiendo que los segmentos de voz presentan una energía significativamente superior frente a los ruidos de fondo o silencios.
    \item \textbf{Zero Crossing Rate (ZCR):} Se computó la frecuencia con la que la señal cambia de signo dentro de la ventana. Aunque el código permite su uso como criterio secundario para filtrar ruidos con alta frecuencia, en la configuración final se priorizó el criterio del STE.
\end{itemize}

El criterio de decisión se estableció mediante un umbral de energía definido. Aquellos tramos con una energía normalizada inferior a $0.02$ ($2\%$ del máximo) se etiquetaron como \emph{Ruido/Silencio}, mientras que las superiores se clasificaron como \emph{Voz/Señal}. Finalmente, se reconstruyó la señal de audio concatenando temporalmente las muestras correspondientes a los segmentos validos, descartando los tramos de silencio y exportando el resultado limpio a la carpeta de destino para su posterior caracterización.

```{r, include=FALSE}
# ZERO CROSSING RATE
zcr_audio <- function(audio, win_time = 0.02, overlap = 50, plot_result = F){
  fs <- audio@samp.rate
  wlen <- floor(win_time*fs)
  
  zcr_calc <- zcr(audio, wl=wlen, ovlp=overlap, plot=plot_result)
  df_zcr <- data.frame(time = zcr_calc[, 1], zcr_value = zcr_calc[, 2])
  return(df_zcr)
}

# SHORT TIME ENERGY
ste_audio <- function(audio, win_time = 0.02, overlap = 50, plot_result = F){
  signal <- audio@left
  fs <- audio@samp.rate
  N <- length(signal)
  
  overlap_pct <- overlap/100
  wlen <- floor(win_time*fs)
  step_size <- floor(wlen * (1 - overlap_pct))
  num_frames <- floor((N - wlen) / step_size) + 1
  ste_vec <- numeric(num_frames)
  time_axis <- numeric(num_frames)
  
  # Cálculo de la energía
  for (i in 1:num_frames) {
    start_idx <- (i - 1) * step_size + 1
    end_idx <- start_idx + wlen - 1
    
    if(end_idx > N) break
    
    window_data <- signal[start_idx:end_idx]
    
    ste_vec[i] <- sum(window_data^2)
    
    time_axis[i] <- (start_idx + wlen/2) / fs
  }
  
  ste_norm <- (ste_vec - min(ste_vec)) / (max(ste_vec) - min(ste_vec))
  
  if (plot_result) {
    plot(time_axis, ste_norm, type = "l", col = "darkgreen",
         main = paste("STE Normalizada - Ventana:", wlen/fs*1000, "ms"),
         xlab = "Tiempo (s)", ylab = "Energía normalizada")
    grid()
  }
  
  df_ste <- data.frame(time = time_axis, ste = ste_norm)
  return(df_ste)
}
```

```{r, include=FALSE}
# DETECCIÓN RUIDO/SILENCIO

noise_audio <- function(audio, win_time = 0.02, overlap = 50, umbral_zcr = 0.05, umbral_ste = 0.02, plot_result = T){
  datos_ste <- ste_audio(audio, win_time, overlap, plot_result = F)
  datos_zcr <- zcr_audio(audio, win_time, overlap, plot_result = F)
  
  # Sincronizar longitudes
  n_filas <- min(nrow(datos_ste), nrow(datos_zcr))
  datos_ste <- datos_ste[1:n_filas, ]
  datos_zcr <- datos_zcr[1:n_filas, ]
  
  df_final <- datos_ste
  df_final$zcr <- datos_zcr$zcr_value
  df_final$es_ruido <- FALSE
  
  # Criterio 1: baja energía = silencio o ruido de fondo
  df_final$es_ruido[df_final$ste < umbral_ste] <- TRUE
  
  # Criterio 2 (opcional): alta ZCR con energía media-baja suele ser ruido de viento o siseo
  #df_final$es_ruido[df_final$zcr > umbral_zcr & df_final$ste < 0.2] <- TRUE
  
  df_final$etiqueta <- ifelse(df_final$es_ruido, "Ruido/Silencio", "Voz/Señal")
  
  if (plot_result) {
    plot(df_final$time, df_final$ste, type = "l", col = "gray", lwd = 2,
         main = "Detección Ruido", 
         xlab = "Tiempo (s)", ylab = "Energía Normalizada", ylim = c(0,1))
    
    # Ruido
    points(df_final$time[df_final$es_ruido], 
           df_final$ste[df_final$es_ruido], 
           col = rgb(1, 0, 0, 0.5), pch = 16, cex = 0.5)
    
    # Señal
    points(df_final$time[!df_final$es_ruido], 
           df_final$ste[!df_final$es_ruido], 
           col = rgb(0, 0.6, 0, 0.5), pch = 16, cex = 0.5)
           
    abline(h = umbral_ste, col = "blue", lty = 2, lwd = 2)
    legend("topright", legend = c("Señal", "Ruido", "Umbral"), 
           col = c("darkgreen", "red", "blue"), pch = 16, lty = c(NA, NA, 2))
  }
  
  return(df_final)
}
```


```{r, include=FALSE}
# ELIMINACIÓN RUIDO/SILENCIO

clean_audio <- function(audio, df_analisis) {
  fs <- audio@samp.rate
  
  # Tiempos donde NO hay ruido
  tiempos_senal <- df_analisis$time[!df_analisis$es_ruido]

  # Concatenamos solo las muestras que corresponden a tramos de la señal
  dt <- df_analisis$time[2] - df_analisis$time[1]
  muestras_por_trama <- floor(dt * fs)
  
  indices_validos <- which(!df_analisis$es_ruido)
  senal_reconstruida <- numeric()
  
  vec_logico <- rep(FALSE, length(audio@left))
  
  for(idx in indices_validos){
    # Convertimos el tiempo del tramo a índice de muestra aproximado
    centro <- df_analisis$time[idx]
    inicio <- round((centro - dt/2) * fs)
    fin <- round((centro + dt/2) * fs)
    
    # Asegurar límites
    inicio <- max(1, inicio)
    fin <- min(length(audio@left), fin)
    
    vec_logico[inicio:fin] <- TRUE
  }
  
  samples_limpios <- audio@left[vec_logico]
  
  # Creamos un nuevo objeto wave
  wave_limpio <- Wave(left = samples_limpios, samp.rate = fs, bit = audio@bit)
  return(wave_limpio)
}
```

# EXTRACCIÓN DE CARACTERÍSTICAS

Las señales de voz contienen mucha información, pero trabajar directamente con el audio no resulta práctico para entrenar modelos de clasificación. Por ello, es necesario extraer una serie de características numéricas que describan la señal de forma más simple y manejable.

La extracción de características permite resumir aspectos importantes de la voz, como su tono, su energía o cómo se distribuye el sonido en distintas frecuencias. De esta manera, cada audio puede representarse mediante un conjunto de valores que capturan sus propiedades principales, evitando depender de la duración del archivo o de pequeñas variaciones que no aportan información relevante.

Estas características sirven como base para entrenar los modelos utilizados en el proyecto, ya que facilitan la comparación entre distintas voces y permiten identificar patrones asociados a diferentes tipos de hablantes o señales. Gracias a este proceso, es posible abordar tareas de clasificación como la identificación del sexo, el acento o la distinción entre voces humanas y generadas por inteligencia artificial.

### PERIOD PITCH

El *period pitch* está directamente relacionado con la frecuencia fundamental de la voz, que corresponde a la vibración periódica más baja producida por las cuerdas vocales y constituye la base sobre la que se construyen el resto de componentes armónicos del habla. La frecuencia fundamental es especialmente relevante porque representa una característica estable de la voz y está estrechamente vinculada a la percepción de si una voz suena más grave o más aguda.

El period pitch se estimó mediante un método basado en la autocorrelación de la señal, que permite detectar la periodicidad dominante en los segmentos sonoros del audio, principalmente en vocales. A partir del retardo correspondiente al máximo de la autocorrelación se obtiene el período fundamental de la señal, y su inversa proporciona una estimación de la frecuencia fundamental o pitch, expresada en hercios (Hz), es decir, vibraciones por segundo.

Diversos estudios en la literatura han documentado rangos característicos de frecuencia fundamental para voces masculinas y femeninas. En particular, Rabiner y Schafer describen que las voces masculinas suelen situarse aproximadamente entre 85 y 180 Hz, mientras que las voces femeninas presentan valores más elevados, entre 165 y 255 Hz (Rabiner, 1978). Resultados similares se reportan en estudios más recientes, donde se observa que las voces masculinas tienden a concentrarse en el rango de 90 a 150 Hz, mientras que las voces femeninas se sitúan aproximadamente entre 190 y 240 Hz (Basu, 2020).

A partir de estos rangos reportados en la literatura, se definieron umbrales prácticos para la clasificación del sexo basados en el valor del \textit{pitch}. Con el fin de evitar clasificaciones forzadas en zonas de solapamiento entre ambos grupos, se estableció una región intermedia de indeterminación. De este modo, se consideraron voces masculinas aquellas con frecuencias fundamentales comprendidas entre 85 y 170 Hz, y voces femeninas aquellas con valores entre 180 y 240 Hz.

Para calcular el pitch creamos la función `period_pitch` con el fin de garantizar la fiabilidad de la estimación, se incorporan una serie de comprobaciones previas sobre la señal de entrada. En primer lugar, se verifica que el audio contenga un número suficiente de muestras, ya que señales excesivamente cortas no permiten identificar una periodicidad clara. Asimismo, la búsqueda del período fundamental se restringe a un rango de retardos coherente con los valores esperados de frecuencia fundamental en voz humana, evitando así detecciones erróneas asociadas a componentes no relevantes de la señal.

En aquellos casos en los que la longitud efectiva de la señal no permite cubrir dicho rango de retardos, el cálculo del pitch se considera no válido. Esta decisión se basa en el hecho de que los métodos basados en autocorrelación solo proporcionan estimaciones fiables en segmentos sonoros suficientemente largos y con una periodicidad bien definida, como ocurre principalmente en las partes vocales del habla.


```{r, include=FALSE}
period_pitch <- function(signal, fs = NULL, fmin = 80, fmax = 300) {

  # --- Convertir Wave a vector numérico ---
  if (inherits(signal, "Wave")) {
    if (is.null(fs)) fs <- signal@samp.rate

    if (isTRUE(signal@stereo)) {
      x <- (as.numeric(signal@left) + as.numeric(signal@right)) / 2
    } else {
      x <- as.numeric(signal@left)
    }

    signal <- x
  } else {
    signal <- as.numeric(signal)
  }

  signal <- signal[is.finite(signal)]
  signal <- signal - mean(signal)

  n <- length(signal)
  if (n < 10) stop("La señal tiene muy pocas muestras.")

  # --- Lags según F0 esperado ---
  lag_min <- floor(fs / fmax)
  lag_max <- ceiling(fs / fmin)
  lag_max <- min(lag_max, n - 1)

  if (lag_min >= lag_max) stop("Señal demasiado corta para estimar pitch.")

  ac <- as.numeric(acf(signal, lag.max = lag_max, plot = FALSE)$acf)
  ac <- ac[-1]

  lag <- which.max(ac[lag_min:lag_max]) + lag_min - 1
  period <- lag / fs
  pitch <- 1 / period

  list(
    period = period,
    pitch = pitch,
    fs = fs,
    n = n
  )
}
```

### Coeficientes Mel (MFCC) y representación de la señal

Como parte de la extracción de características espectrales, se calcularon los coeficientes cepstrales en escala Mel (MFCC), ampliamente utilizados en tareas de análisis de voz por su capacidad de capturar información relacionada con el timbre y la envolvente espectral. Para ello, se aplicó el procedimiento estándar por ventanas cortas: se dividió cada audio en tramas solapadas (ventana de 25 ms y salto de 10 ms), se realizó la transformada rápida de Fourier (FFT) por trama y se obtuvo el espectro de potencia. Posteriormente, dicho espectro se proyectó sobre un banco de filtros en escala Mel (40 bandas), se aplicó una compresión logarítmica y, finalmente, una transformada discreta del coseno (DCT) para obtener un conjunto compacto de coeficientes (12 MFCC por trama). Adicionalmente, se aplicó un pre-énfasis para compensar la caída de energía en altas frecuencias típica de señales de voz.

Con el fin de incorporar información dinámica, se calcularon también las derivadas temporales de primer orden ($\Delta$) y segundo orden ($\Delta\Delta$), que aproximan respectivamente la \emph{velocidad} y la \emph{aceleración} de los MFCC a lo largo del tiempo. 

Para adaptar esta representación por tramas a modelos clásicos de aprendizaje automático, cada audio se transformó en un único vector de características agregando estadísticas globales: media y desviación típica de los MFCC, así como de sus derivadas $\Delta$ y $\Delta\Delta$. De este modo, cada clip queda representado por un vector fijo que resume tanto el contenido espectral promedio como su variabilidad temporal.


### Centroide espectral

#### Centroide espectral global

El centroide espectral global se calculó como una medida resumen de la distribución energética del espectro de cada señal de audio. Este descriptor representa la frecuencia media ponderada por la potencia espectral y está relacionado con la percepción de brillo del sonido. Para su cálculo, se eliminó previamente el componente de continua de la señal con el fin de evitar un sesgo artificial hacia bajas frecuencias. 

El análisis del centroide espectral permitió observar diferencias en los valores medios entre voces humanas y voces generadas por inteligencia artificial, especialmente al separar los resultados por sexo. No obstante, aunque este descriptor mostró cierto potencial discriminativo, su uso de forma aislada resultó limitado debido al solapamiento existente entre clases.

```{r}
spectral_centroid <- function(w) {
  fs <- as.numeric(w@samp.rate)
  x  <- as.numeric(w@left)

  # quitar media para estabilidad
  x <- x - mean(x)

  N <- length(x)

  X <- fft(x)
  K <- floor(N / 2) + 1

  P <- Mod(X[1:K])^2 #potencia espectral
  denom <- sum(P) #denominador del centroide
  if (!is.finite(denom) || denom <= 0) return(0)

  # Evitar overflow: usar double desde el principio
  freqs <- as.numeric(0:(K - 1)) * (fs / as.numeric(N))

  sum(freqs * P) / denom
}
```


#### Segmentación temporal en 12 partes

Con el objetivo de analizar la evolución temporal del centroide espectral y evaluar la estabilidad espectral de las señales, se propuso una segmentación temporal de cada audio en doce partes consecutivas de igual duración. 

Para cada segmento se calculó de forma independiente el centroide espectral, obteniendo así un vector de doce valores por señal. Esta aproximación permite capturar variaciones temporales del contenido espectral que no son visibles al emplear un único centroide global. A partir de estos valores se derivaron medidas estadísticas como la desviación típica y las diferencias entre centroides consecutivos, con el fin de caracterizar la variabilidad espectral de cada voz. 

Aunque estos descriptores no mostraron una separación clara entre voces humanas y sintéticas cuando se analizaron de forma individual, su combinación permitió capturar patrones más complejos que resultaron útiles en modelos de clasificación no lineales.

```{r}
spectral_centroid_12 <- function(w, n_parts = 12) {
  fs <- as.numeric(w@samp.rate)
  x  <- as.numeric(w@left)
  x  <- x - mean(x) #evita que se sesgue el centroide a frecuencias bajas

  N <- length(x)
  if (N < n_parts * 10 || fs <= 0) return(rep(NA_real_, n_parts)) #por si el audio es demasiado corto

  seg_len <- floor(N / n_parts) #número de muestras por segmento
  cents <- rep(NA_real_, n_parts)

  for (i in seq_len(n_parts)) {
    start <- (i - 1) * seg_len + 1
    end   <- if (i < n_parts) i * seg_len else N
    seg <- x[start:end]
    seg <- seg - mean(seg)

    L <- length(seg)
    if (L < 2) next #por si el segmento es demasiado corto

    X <- fft(seg)
    K <- floor(L / 2) + 1
    P <- Mod(X[1:K])^2
    denom <- sum(P)
    if (!is.finite(denom) || denom <= 0) next

    freqs <- as.numeric(0:(K - 1)) * (fs / as.numeric(L))
    cents[i] <- sum(freqs * P) / denom
  }

  cents
}
```

# CONSTRUCCIÓN DEL CONJUNTO DE DATOS

Para organizar y analizar los 466 audios, se creó un dataframe que contiene las características y etiquetas de cada audio, donde denotamos: 

\begin{itemize}
\item M: masculino / F: femenino
\item P: persona / IA: IA
\item AN: andaluz / AR: argentino / N: neutro
\end{itemize}

Primero, se creó un dataframe que etiquetara cada persona con su sexo, origen y acento, mediante la función `etiquetar_por_nombre()`, que compara el nombre del archivo con los nombres del dataframe de reglas y devuelve las etiquetas correspondientes.

```{r, include = FALSE}
reglas_nombres <- data.frame(
  nombre = c("alvaro", "Oscar", "Iyan", "MAngeles", "aza","Flor","Beatriz","Enrique",
             "Alejandra","Alice","Aurora","Bill","Callum","HermanoFlor","Jessica","Luis","Macarena","Rafael","Tito"),
  sexo   = c("M","M",     "M",    "F",     "F", "F","F","M",
             "F","F","F","M","M","M","F","M","F","M","M"),
  origen = c("P","P","P",    "P",     "P","P","IA","IA",
             "IA","IA","IA","IA","IA","P","IA","IA","IA","IA","IA"),
  acento=c("AN","N","N","AN","N","AR","N","N",
           "N","N","N","N","N","AR","N","N","N","N","N"),
  stringsAsFactors = FALSE
)

reglas_nombres$nombre <- tolower(gsub("\\s+", "", reglas_nombres$nombre))

```

```{r, include = FALSE}
etiquetar_por_nombre <- function(nombre_archivo, reglas) {
  
  for (i in seq_len(nrow(reglas))) {
    if (grepl(reglas$nombre[i], nombre_archivo)) {
      return(list(
        sexo = reglas$sexo[i],
        origen = reglas$origen[i],
        acento=reglas$acento[i]
      ))
    }
  }
  
  return(list(
    sexo = NA,
    origen = NA,
    acento=NA
  ))
}
```


A continuación, para cada audio se incluyeron las siguientes variables: duración, zcr, energía rms y pitch. Para ello se utilizó la función `añadir_voz()` la cuál recibe un archivo de audio, extrae sus características, asigna las etiquetas usando la función anterior y añade toda la información como una nueva fila al dataframe. Así, cada fila del dataframe corresponde a un audio individual, con sus características numéricas y etiquetas.

```{r, include=FALSE}
voces_df <- data.frame(
  señal = character(),
  sexo = character(),
  origen = character(),
  acento = character(),
  duracion = numeric(),
  zcr = numeric(),
  energia_rms = numeric(),
  pitch = numeric(),        
  stringsAsFactors = FALSE
)


añadir_voz <- function(df, ruta_wav) {
  
  # Leemos el audio
  audio <- readWave(ruta_wav)
  fs <- audio@samp.rate
  señal_audio <- audio@left
  
  # Duración
  duracion <- length(señal_audio) / fs
  
  # Zero Crossing Rate
 zcr <- mean(zcr(señal_audio, f = fs,plot = F))
  
  # Energía RMS
  energia_rms <- sqrt(mean(señal_audio^2))
  
  # Etiquetado 
  nombre_archivo <- tolower(basename(ruta_wav))
  nombre_archivo <- gsub("\\s+", "", nombre_archivo)

  etiquetas <- etiquetar_por_nombre(nombre_archivo, reglas_nombres)

  sexo <- etiquetas$sexo
  origen <- etiquetas$origen
  acento<-etiquetas$acento
  
  #PERIOD PITCH:
  pitch_res <- period_pitch(audio)
  pitch <- pitch_res$pitch
  
  # Nueva fila
  nueva_fila <- data.frame(
    señal = basename(ruta_wav),
    sexo = sexo,
    origen=origen,
    acento = acento,
    duracion = duracion,
    zcr = zcr,
    energia_rms = energia_rms,
    period_pitch = pitch,
    stringsAsFactors = FALSE
  )
  
  df <- rbind(df, nueva_fila)
  return(df)
}


```

Cabe destacar que todas las características consideradas en el conjunto de datos están definidas como medias o medidas normalizadas respecto a la longitud de la señal, de modo que no dependen de la duración total del audio. En consecuencia, el proceso de eliminación de silencios únicamente reduce las regiones no informativas de la señal, sin afectar a la coherencia ni a la comparabilidad de las características extraídas entre distintos audios.



Para automatizar la creación del dataframe, se implementó la función `df_carpeta()`, que recorre todos los archivos de la carpeta con los audios limpios y aplica `añadir_voz()` a cada uno. De forma que, se obtiene un  dataframe completo, voces_df, con toda la información lista para análisis y clasificación.

```{r, include=FALSE}
df_carpeta <- function(df, carpeta) {
  archivos <- list.files(
    path = carpeta,
    pattern = "\\.wav$",
    full.names = TRUE
  )
  
  for (ruta in archivos) {
    df <- añadir_voz(df, ruta)
  }
  
  return(df)
}

```

```{r, include=FALSE}
voces_df <- df_carpeta(voces_df, "Audios_finales_limp")
```


# ANÁLISIS EXPLORATORIO

Antes de aplicar los métodos de clasificación, se realizó un análisis exploratorio del conjunto de datos con el objetivo de comprender la distribución de las variables y detectar posibles patrones relevantes. Así, el conjunto de datos final está compuesto por 466 audios, con una distribución equilibrada entre voces femeninas (240) y masculinas (226), lo que reduce la posibilidad de sesgos derivados del desbalanceo de clases.

```{r, include=FALSE}
table(voces_df$sexo)
```

Tras hacer un análisis descriptivo usando `summary()`, se puede ver que la duración de los audios después de limpiarlos y eliminar los silencios presenta una variabilidad moderada, con valores comprendidos aproximadamente entre 1.5 y 5 segundos. Por otro lado, las características ZCR y energía RMS presentan rangos consistentes y valores medios estables, indicando señales con contenido sonoro continuo y sin presencia dominante de ruido. 

```{r, include=FALSE}
summary(voces_df[, c("duracion", "zcr", "energia_rms", "period_pitch")])
```

En cuanto a la frecuencia fundamental (pitch), se observa un rango amplio de valores (aproximadamente entre 80 y 285 Hz), consistente con voces humanas. El análisis por sexo revela que las voces femeninas presentan valores de pitch más elevados que las masculinas. Si lo representamos usando un diagrama de cajas, podemos ver un solapamiento entre ambas distribuciones, lo que indica que probablemente existan errores de clasificación en algunos audios concretos al emplear únicamente esta característica.

```{r, echo=FALSE}
boxplot(period_pitch ~ sexo,
        data = voces_df,
        main = "Distribución del pitch según el sexo",
        ylab = "Pitch (Hz)",
        xlab = "Sexo")
```

Por último, el análisis de correlación entre las variables numéricas muestra una alta correlación entre la duración y la ZCR, mientras que el pitch presenta una correlación prácticamente nula con el resto de variables. Esto sugiere que el pitch aporta información complementaria y relevante para la tarea de clasificación.

```{r, echo=FALSE}
vars_num <- voces_df[, c("duracion", "zcr", "energia_rms", "period_pitch")]
round(cor(vars_num, use = "complete.obs"), 2)
```


# MODELOS DE CLASIFICACIÓN Y VALIDACIÓN

## Clasificación del sexo

A partir del conjunto de características extraídas y del análisis exploratorio realizado, tenemos como objetivo la clasificación del sexo del hablante. De esta forma, se plantea el problema como una clasificación binaria y se emplea un modelo de regresión logística. Este tipo de modelo permite relacionar las características extraídas con la probabilidad de que una voz corresponda a un hablante masculino o femenino. 

Para ello, se codificó la variable objetivo \texttt{sexo} en formato numérico, asignando el valor 1 a las voces femeninas y 0 a las masculinas. Esta transformación permite el uso de modelos probabilísticos basados en regresión logística.

Inicialmente, se ajustó un modelo de regresión logística multivariante que incluía todas las características disponibles en el conjunto de datos: duración, tasa de cruces por cero (ZCR), energía RMS, frecuencia fundamental (pitch), así como las variables categóricas de origen y acento. Este primer modelo tiene como objetivo evaluar la contribución conjunta de todas las variables y explorar posibles relaciones entre ellas y la variable respuesta.

```{r, include=FALSE}
voces_df$gender <- ifelse(voces_df$sexo == "F", 1, 0)
table(voces_df$gender)

voces_modif<-voces_df[-1]
voces_modif$origen <- factor(voces_modif$origen)
voces_modif$acento <- factor(voces_modif$acento)

mod <- glm(gender ~ ., data = voces_modif, family = binomial(link="logit"))
summary(mod)

(mod$null.deviance - mod$deviance)/mod$null.deviance
par(mfrow = c(2,2))
```

Al analizar el resumen del modelo de regresión logística completo mediante la función `summary()`, se observó que la única variable estadísticamente significativa para la clasificación del sexo era la frecuencia fundamental (period pitch), mientras que el resto de variables no aportaban evidencia significativa una vez incorporado dicho descriptor. A partir de esta observación, se decidió contrastar un modelo reducido basado exclusivamente en el pitch frente al modelo completo mediante un ANOVA, con el fin de comprobar si la complejidad adicional estaba justificada.

```{r, echo=FALSE}
mod_simple <- glm(gender ~ period_pitch, data = voces_modif, family = binomial)
anova(mod_simple, mod, test = "Chisq")
```
El contraste mediante ANOVA entre el modelo reducido y el modelo completo muestra que incorporar el resto de variables produce una reducción significativa de la devianza (p < 0.001). No obstante, el análisis individual de los coeficientes revela que únicamente el period pitch resulta estadísticamente significativo, mientras que el resto de variables no aportan información relevante de forma independiente.


A continuación, se evaluó el desempeño del modelo reducido en un esquema de entrenamiento y prueba. Para ello, se seleccionó aleatoriamente un $70\%$ de las observaciones para ajustar el modelo y se reservó el $30\%$ para validación.

```{r, include=FALSE}
set.seed(0)
train <- sample(nrow(voces_modif),0.7 * nrow (voces_modif))
mod_pred <- glm(gender ~ period_pitch, family = binomial ( link = logit ) , data =voces_modif[ train ,])
summary ( mod_pred)

(mod_pred$null.deviance - mod_pred$deviance)/mod_pred$null.deviance
```

La bondad de ajuste del modelo se reflejó en un pseudo-R² de 0.697, lo que sugiere que aproximadamente el $69.7\%$  de la variabilidad del conjunto de entrenamiento queda explicada por el modelo basado únicamente en el period pitch. Posteriormente, se aplicó el modelo al conjunto de prueba para evaluar su capacidad predictiva.

```{r, include=FALSE}
pred <- predict(mod_pred, voces_modif[-train,], type = "response")
cor(pred, voces_modif$gender[-train])
```

La correlación entre las probabilidades predichas y las clases reales alcanzó un valor de 0.918, evidenciando una buena concordancia entre las predicciones y las etiquetas de sexo. 

```{r, echo=FALSE}
plot(pred, voces_modif$gender[-train], pch = 20)
```

Al observar la gráfica de predicciones, se aprecia que la mayoría de los puntos correspondientes a voces masculinas se agrupan cerca de 0, mientras que las voces femeninas se concentran alrededor de 1, lo que indica que el modelo reconoce correctamente la mayoría de los casos. Sin embargo, algunos puntos se sitúan en valores intermedios, mostrando incertidumbre en la predicción.

Sin embargo, en el análisis exploratorio vimos que el diagrama de cajas del pitch tenía zonas donde había solapamiento entre los rangos de frecuencia fundamental de hombres y mujeres. Por lo tanto, es probable que los audios con pitch en el rango intermedio sean precisamente los que generan errores de clasificación, reflejando la variabilidad natural del tono de voz y características individuales que no se capturan únicamente con el period pitch.

En conjunto, estos resultados refuerzan que el pitch es el descriptor más relevante para la clasificación del sexo, pero que la superposición natural de frecuencias humanas introduce un margen de error inevitable, explicando los puntos intermedios en la gráfica de predicciones.


## Clasificación del acento

Además de la clasificación por sexo, se abordó la identificación del acento de la voz, considerando tres categorías: español neutro, andaluz y argentino. Para esta tarea se empleó un modelo de regresión logística multinomial, adecuado para problemas de clasificación con más de dos clases.

Como variables predictoras se utilizaron descriptores acústicos de bajo nivel, concretamente la tasa de cruces por cero (ZCR), la energía RMS y la duración del audio. Estas características se seleccionaron por su relación con aspectos articulatorios y prosódicos que pueden variar entre acentos.

Con el fin de explorar la separabilidad entre clases, se realizó un análisis gráfico de la distribución del ZCR en función del acento, observándose diferencias apreciables entre algunas categorías. Posteriormente, se ajustó el modelo multinomial y se evaluó su capacidad explicativa mediante el pseudo-$R^2$ de McFadden, comparando la verosimilitud del modelo completo con la de un modelo nulo.

Aunque los resultados muestran que estas características contienen información relevante para la clasificación del acento, el rendimiento del modelo está condicionado por el tamaño reducido del conjunto de datos, por lo que se considera necesario ampliar el número de muestras para obtener conclusiones más robustas.


## Clasificación del origen (IA vs Humano)


