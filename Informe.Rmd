---
title: "Informe"
author: "Azahara Martinez"
date: "2025-12-27"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# OBJETIVOS DEL PROYECTO

# PREPROCESADO

\subsection{Conversión y lectura de audios}

Como primer paso del preprocesado, se desarrolló una función para la conversión de archivos de audio desde el formato \texttt{.m4a} al formato \texttt{.wav}, con el objetivo de unificar el tipo de señal y facilitar su posterior análisis. Para ello, se emplearon las librerías \texttt{av} y \texttt{tuneR} de \textsf{R}, que permiten la conversión y lectura de señales de audio de forma eficiente.

```{r}
# Convierte un archivo .m4a a .wav y lo guarda en la carpeta "Audioswav_prueba"
# Parámetros:
#  - input_m4a: ruta del archivo .m4a de entrada
#  - sample_rate: frecuencia de muestreo del audio de salida (Hz)

convertir_m4a_a_wave <- function(input_m4a, sample_rate = 16000) {
  # Crear carpeta de salida si no existe
  dir.create("Audioswav_prueba", showWarnings = FALSE)
  
  # Nombre del archivo sin extensión
  base_name <- tools::file_path_sans_ext(basename(input_m4a))
  
  # Ruta de salida dentro de Audioswav_prueba
  output_wav <- file.path("Audioswav_prueba", paste0(base_name, ".wav"))
  
  # OJO: llamada por posición, sin "input =" ni "output ="
  av_audio_convert(
    input_m4a,        # archivo de entrada
    output_wav,       # archivo de salida
    channels    = 1,
    sample_rate = sample_rate
  )
  
  return(output_wav)
}

# Carpeta raíz donde están los audios
input_dir <- "AudiosOriginales"

# Lista de TODOS los .m4a dentro de la carpeta y subcarpetas
m4a_files <- list.files(
  path       = input_dir,
  pattern    = "\\.m4a$",
  recursive  = TRUE,     # <-- incluye subcarpetas (como "Voz IA")
  full.names = TRUE
)

# Convertir todos a wav dentro de "Audioswav"
rutas_wav <- sapply(m4a_files, convertir_m4a_a_wave)

rutas_wav  # aquí tienes las rutas de los wav generados

#IMPORTANTE: LOS AUDIOS IA NO LOS COGE PORQUE YA ESTAN EN WAV: Podriamos simplemente copiarlos y pegarlos en la carpeta final donde estaran los audios. 
```

La función implementada convierte cada archivo a una señal monofónica con una frecuencia de muestreo de 16 kHz y guarda los archivos resultantes en un directorio específico. Además, se automatizó el proceso para convertir de manera recursiva todos los archivos \texttt{.m4a} contenidos en una carpeta y sus subdirectorios.

\subsection{Detección y eliminación de ruido y silencios}

Con el objetivo de mejorar la calidad de las señales de voz antes de la extracción de características, se implementó un módulo de detección y eliminación de ruido/silencios basado en medidas temporales por tramas. El audio se segmenta en ventanas cortas (20 ms) con solapamiento del 50\%, lo que permite analizar de forma local la actividad de la señal.

Para cada trama se calcularon dos descriptores:
\begin{itemize}
    \item \textbf{Zero Crossing Rate (ZCR):} tasa de cruces por cero, asociada al contenido de alta frecuencia y útil para caracterizar señales ruidosas o fricativas.
    \item \textbf{Energía de corto tiempo (STE):} suma de cuadrados de la señal en cada ventana, normalizada en el intervalo $[0,1]$, empleada como indicador de presencia de voz frente a silencio o fondo.
\end{itemize}

La detección de ruido se realizó aplicando un criterio umbral sobre la energía: aquellas tramas con energía inferior a un umbral predefinido se etiquetaron como \emph{Ruido/Silencio}, mientras que el resto se consideraron \emph{Voz/Señal}. De forma opcional, se contempló un segundo criterio basado en ZCR para detectar segmentos con alta tasa de cruces por cero y energía baja-media, característicos de siseos o ruido de viento.

A partir de la máscara temporal resultante, se reconstruyó una señal limpia concatenando únicamente las muestras correspondientes a tramas etiquetadas como voz. Finalmente, se compararon visualmente las formas de onda original y procesada y se exportó el audio resultante en formato \texttt{.wav} para su uso en las etapas posteriores del sistema.

# EXTRACCIÓN DE CARACTERÍSTICAS

\subsection{Period pitch}

Con el objetivo de diferenciar entre voces masculinas y femeninas, se empleó el *period pitch* de la señal de voz como descriptor principal. Este parámetro está directamente relacionado con la frecuencia fundamental de la voz, que corresponde a la vibración periódica más baja producida por las cuerdas vocales y constituye la base sobre la que se construyen el resto de componentes armónicos del habla. La frecuencia fundamental es especialmente relevante porque representa una característica estable de la voz y está estrechamente vinculada a la percepción de si una voz suena más grave o más aguda.

El period pitch se estimó mediante un método basado en la autocorrelación de la señal, que permite detectar la periodicidad dominante en los segmentos sonoros del audio, principalmente en vocales. A partir del retardo correspondiente al máximo de la autocorrelación se obtiene el período fundamental de la señal, y su inversa proporciona una estimación de la frecuencia fundamental o pitch, expresada en hercios (Hz), es decir, vibraciones por segundo.

Diversos estudios en la literatura han documentado rangos característicos de frecuencia fundamental para voces masculinas y femeninas. En particular, Rabiner y Schafer describen que las voces masculinas suelen situarse aproximadamente entre 85 y 180 Hz, mientras que las voces femeninas presentan valores más elevados, entre 165 y 255 Hz \cite{rabiner1978}. Resultados similares se reportan en estudios más recientes, donde se observa que las voces masculinas tienden a concentrarse en el rango de 90 a 150 Hz, mientras que las voces femeninas se sitúan aproximadamente entre 190 y 240 Hz \cite{basu2020}.

A partir de estos rangos reportados en la literatura, se definieron umbrales prácticos para la clasificación del sexo basados en el valor del \textit{pitch}. Con el fin de evitar clasificaciones forzadas en zonas de solapamiento entre ambos grupos, se estableció una región intermedia de indeterminación. De este modo, se consideraron voces masculinas aquellas con frecuencias fundamentales comprendidas entre 85 y 170 Hz, y voces femeninas aquellas con valores entre 180 y 240 Hz.



```{r}
period_pitch <- function(signal, fs = NULL, fmin = 80, fmax = 300) {

  # --- Convertir Wave a vector numérico ---
  if (inherits(signal, "Wave")) {
    if (is.null(fs)) fs <- signal@samp.rate

    if (isTRUE(signal@stereo)) {
      x <- (as.numeric(signal@left) + as.numeric(signal@right)) / 2
    } else {
      x <- as.numeric(signal@left)
    }

    signal <- x
  } else {
    signal <- as.numeric(signal)
  }

  signal <- signal[is.finite(signal)]
  signal <- signal - mean(signal)

  n <- length(signal)
  if (n < 10) stop("La señal tiene muy pocas muestras.")

  # --- Lags según F0 esperado ---
  lag_min <- floor(fs / fmax)
  lag_max <- ceiling(fs / fmin)
  lag_max <- min(lag_max, n - 1)

  if (lag_min >= lag_max) stop("Señal demasiado corta para estimar pitch.")

  ac <- as.numeric(acf(signal, lag.max = lag_max, plot = FALSE)$acf)
  ac <- ac[-1]

  lag <- which.max(ac[lag_min:lag_max]) + lag_min - 1
  period <- lag / fs
  pitch <- 1 / period

  # --- Clasificación por pitch ---
  gender <- if (pitch < 170) {
    "Masculina"
  } else if (pitch > 180) {
    "Femenina"
  } else {
    "Indeterminado"
  }

  list(
    period = period,
    pitch = pitch,
    gender = gender,
    fs = fs,
    n = n
  )
}
```

\subsection{ZCR/Energía}

\subsection{Coeficientes Mel (MFCC) y representación de la señal}

Como parte de la extracción de características espectrales, se calcularon los coeficientes cepstrales en escala Mel (MFCC), ampliamente utilizados en tareas de análisis de voz por su capacidad de capturar información relacionada con el timbre y la envolvente espectral. Para ello, se aplicó el procedimiento estándar por ventanas cortas: se dividió cada audio en tramas solapadas (ventana de 25 ms y salto de 10 ms), se realizó la transformada rápida de Fourier (FFT) por trama y se obtuvo el espectro de potencia. Posteriormente, dicho espectro se proyectó sobre un banco de filtros en escala Mel (40 bandas), se aplicó una compresión logarítmica y, finalmente, una transformada discreta del coseno (DCT) para obtener un conjunto compacto de coeficientes (12 MFCC por trama). Adicionalmente, se aplicó un pre-énfasis para compensar la caída de energía en altas frecuencias típica de señales de voz.

Con el fin de incorporar información dinámica, se calcularon también las derivadas temporales de primer orden ($\Delta$) y segundo orden ($\Delta\Delta$), que aproximan respectivamente la \emph{velocidad} y la \emph{aceleración} de los MFCC a lo largo del tiempo. 

Para adaptar esta representación por tramas a modelos clásicos de aprendizaje automático, cada audio se transformó en un único vector de características agregando estadísticas globales: media y desviación típica de los MFCC, así como de sus derivadas $\Delta$ y $\Delta\Delta$. De este modo, cada clip queda representado por un vector fijo que resume tanto el contenido espectral promedio como su variabilidad temporal.

#TODO faltaría añadir para qué lo hemos usado al final (también podemos explicar que ibámos a usarlo para distinguir IA de humano pero no salía muy allá)

\subsection{Centroide espectral}

Con el objetivo de diferenciar entre voces humanas y voces generadas por inteligencia artificial, se analizó el centroide espectral de las señales de audio. Este descriptor proporciona una medida de la frecuencia media ponderada por la energía del espectro y está relacionado con la distribución espectral de la señal.

El análisis mostró que las voces generadas por IA tienden a presentar centroides espectrales más estables y, en muchos casos, más bajos que los de las voces humanas. Esto se debe a que las señales sintéticas suelen carecer de micro-ruido, irregularidades y fenómenos de fricación propios de la producción vocal humana, presentando un espectro más limpio y concentrado en determinadas bandas de frecuencia. Por el contrario, la voz humana exhibe una mayor dispersión espectral debido a la respiración, turbulencia y variabilidad natural del habla.

A partir de estas observaciones, se estudió la separabilidad entre clases utilizando diagramas de caja, diferenciando además por sexo. En el caso de las voces masculinas, se observó una clara separación entre las clases humana e IA, sin solapamiento apreciable, lo que permitiría una clasificación casi perfecta mediante un umbral adecuado. En cambio, para las voces femeninas se detectó cierto solapamiento entre ambas clases, implicando la posible aparición de errores de clasificación.

Para formalizar este proceso, se calibraron umbrales óptimos del centroide espectral de manera independiente para voces masculinas y femeninas, maximizando métricas de rendimiento como la exactitud y la medida F1. Finalmente, se evaluó el rendimiento del clasificador basado en umbrales mediante una matriz de confusión, confirmando un buen desempeño global, especialmente en el caso de las voces masculinas.

#TODO faltaría añadir el código

# CONSTRUCCIÓN DEL CONJUNTO DE DATOS

Una vez convertidos los audios al formato \texttt{.wav}, se procedió a la construcción de un conjunto de datos estructurado que permitiese almacenar, para cada señal de voz, tanto sus características acústicas como sus etiquetas asociadas. Para ello, se definió un \textit{dataframe} donde cada fila representa un audio y cada columna una variable relevante para el análisis.

```{r}

```


Las etiquetas de sexo, origen (persona o voz generada por IA) y acento se asignaron de forma automática a partir del nombre del archivo, utilizando un conjunto de reglas previamente definidas. Este enfoque permitió etiquetar los datos de manera consistente sin necesidad de intervención manual.

Para cada señal se extrajeron distintas características acústicas de bajo nivel, entre las que se incluyen la duración del audio, la tasa de cruces por cero (ZCR) y la energía RMS, utilizadas como descriptores básicos de la señal. Asimismo, se calculó el \textit{period pitch} mediante un análisis de autocorrelación, obteniendo una estimación de la frecuencia fundamental de la voz, que posteriormente se empleó en tareas de clasificación de género.

Adicionalmente, se calculó el centroide espectral de cada señal, una medida que describe la distribución de la energía en el dominio frecuencial. Este parámetro resultó especialmente útil para analizar diferencias entre voces humanas y voces generadas por sistemas de inteligencia artificial, debido a las variaciones en la dispersión espectral y la presencia de irregularidades propias de la voz humana.

#TODO faltaría añadir el código


# MODELOS DE CLASIFICACIÓN

\subsection{Clasificación del sexo}


\subsection{Clasificación del acento}

Además de la clasificación por sexo, se abordó la identificación del acento de la voz, considerando tres categorías: español neutro, andaluz y argentino. Para esta tarea se empleó un modelo de regresión logística multinomial, adecuado para problemas de clasificación con más de dos clases.

Como variables predictoras se utilizaron descriptores acústicos de bajo nivel, concretamente la tasa de cruces por cero (ZCR), la energía RMS y la duración del audio. Estas características se seleccionaron por su relación con aspectos articulatorios y prosódicos que pueden variar entre acentos.

Con el fin de explorar la separabilidad entre clases, se realizó un análisis gráfico de la distribución del ZCR en función del acento, observándose diferencias apreciables entre algunas categorías. Posteriormente, se ajustó el modelo multinomial y se evaluó su capacidad explicativa mediante el pseudo-$R^2$ de McFadden, comparando la verosimilitud del modelo completo con la de un modelo nulo.

Aunque los resultados muestran que estas características contienen información relevante para la clasificación del acento, el rendimiento del modelo está condicionado por el tamaño reducido del conjunto de datos, por lo que se considera necesario ampliar el número de muestras para obtener conclusiones más robustas.

#TODO faltaría añadir el código

\subsection{Clasificación del origen (IA vs Humano)}


# VALIDACIÓN DE LOS MODELOS
